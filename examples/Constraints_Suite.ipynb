{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8db204",
   "metadata": {},
   "source": [
    "# Simple constraint examples and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13777a7",
   "metadata": {},
   "source": [
    "#### There is a specific function for common constraints. Should only continue to use the ValueConstraint and SummaryConstraint for creating a custom constraint that can't be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32c0dc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Missing config\n"
     ]
    }
   ],
   "source": [
    "from whylogs import get_or_create_session\n",
    "from whylogs.util.protobuf import message_to_json\n",
    "\n",
    "# create session\n",
    "session = get_or_create_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0591c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ba8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def indent(txt, spaces=4):\n",
    "    return \"\\n\".join(\" \" * spaces + ln for ln in txt.splitlines())\n",
    "\n",
    "def format_report(r):\n",
    "    # report failures in tabular form\n",
    "    \n",
    "    r_2 = [entry for entry in r if len(entry)==2] # all the single column constraints\n",
    "    r_table_shape = [[entry for entry in r if len(entry)!=2 and entry[0].startswith(\"table\")]] # multi column and table shape constraints\n",
    "    r_multi_column = [[entry for entry in r if len(entry)!=2 and entry[0].startswith(\"multi column\")]]\n",
    "    \n",
    "    if len(r_2):\n",
    "        print(\"Constraint failures by feature - \")\n",
    "    for c,r in r_2:\n",
    "        print(f\"{c}:\")\n",
    "        if len(r[0][0]) > 80: \n",
    "            print(f\"\\ntest_name:\\t{r[0][0]}\\n\")\n",
    "            print(f\"total_run:\\t{r[0][1]}\\n\")\n",
    "            print(f\"failed:\\t\\t{r[0][2]}\\n\")\n",
    "        else:    \n",
    "            print(indent(tabulate(r, tablefmt=\"plain\", headers=['test_name', 'total_run', 'failed'])))\n",
    "    \n",
    "    if len(r_table_shape[0]):\n",
    "        print ()   \n",
    "        print(\"Table shape constraint failures -\")\n",
    "        for entry in r_table_shape:\n",
    "            print(indent(tabulate(entry, tablefmt=\"plain\", headers=['test_name', 'total_run', 'failed'])))\n",
    "        \n",
    "    if len(r_multi_column[0]):\n",
    "        print()    \n",
    "        print(\"Multi column constraint failures -\")\n",
    "        for entry in r_multi_column:\n",
    "            if len(entry[0][0]) > 80: \n",
    "                print(f\"\\ntest_name:\\t{entry[0][0]}\\n\")\n",
    "                print(f\"total_run:\\t{entry[0][1]}\\n\")\n",
    "                print(f\"failed:\\t\\t{entry[0][2]}\\n\")\n",
    "            else:\n",
    "                print(indent(tabulate(entry, tablefmt=\"plain\", headers=['test_name', 'total_run', 'failed'])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6068d2",
   "metadata": {},
   "source": [
    "## Between summary constraints on summary fields like: stddev, min, max, mean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd0b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import (\n",
    "    maxBetweenConstraint,\n",
    "    maxLessThanEqualConstraint,\n",
    "    meanBetweenConstraint,\n",
    "    minBetweenConstraint,\n",
    "    minGreaterThanEqualConstraint,\n",
    "    stddevBetweenConstraint,\n",
    "    stringLengthBetweenConstraint,\n",
    "    stringLengthEqualConstraint,\n",
    "    quantileBetweenConstraint,\n",
    "    DatasetConstraints,\n",
    "    SummaryConstraints,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c0b923",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints for column 'col1': \n",
      "[\n",
      "    {\n",
      "        \"name\": \"summary max BTWN 5 and 10.8\",\n",
      "        \"firstField\": \"max\",\n",
      "        \"op\": \"BTWN\",\n",
      "        \"between\": {\n",
      "            \"lowerValue\": 5.0,\n",
      "            \"upperValue\": 10.8\n",
      "        },\n",
      "        \"verbose\": false,\n",
      "        \"quantileValue\": 0.0\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"standard deviation is between 2.3 and 5.4\",\n",
      "        \"firstField\": \"stddev\",\n",
      "        \"op\": \"BTWN\",\n",
      "        \"between\": {\n",
      "            \"lowerValue\": 2.3,\n",
      "            \"upperValue\": 5.4\n",
      "        },\n",
      "        \"verbose\": false,\n",
      "        \"quantileValue\": 0.0\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"minimum is greater than or equal to 1\",\n",
      "        \"firstField\": \"min\",\n",
      "        \"value\": 1.0,\n",
      "        \"op\": \"GE\",\n",
      "        \"verbose\": false,\n",
      "        \"quantileValue\": 0.0\n",
      "    }\n",
      "]\n",
      "\n",
      "Constraints for column 'col2': \n",
      "[\n",
      "    {\n",
      "        \"name\": \"mean is between 1.2 and 1.6\",\n",
      "        \"firstField\": \"mean\",\n",
      "        \"op\": \"BTWN\",\n",
      "        \"between\": {\n",
      "            \"lowerValue\": 1.2,\n",
      "            \"upperValue\": 1.6\n",
      "        },\n",
      "        \"verbose\": false,\n",
      "        \"quantileValue\": 0.0\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"minimum is between 0.1 and 0.5\",\n",
      "        \"firstField\": \"min\",\n",
      "        \"op\": \"BTWN\",\n",
      "        \"between\": {\n",
      "            \"lowerValue\": 0.1,\n",
      "            \"upperValue\": 0.5\n",
      "        },\n",
      "        \"verbose\": false,\n",
      "        \"quantileValue\": 0.0\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"0.15-th quantile value is between 2 and 4.3\",\n",
      "        \"firstField\": \"quantile\",\n",
      "        \"op\": \"BTWN\",\n",
      "        \"between\": {\n",
      "            \"lowerValue\": 2.0,\n",
      "            \"upperValue\": 4.3\n",
      "        },\n",
      "        \"quantileValue\": 0.15,\n",
      "        \"verbose\": false\n",
      "    }\n",
      "]\n",
      "\n",
      "Constraints for column 'col3': \n",
      "[\n",
      "    {\n",
      "        \"name\": \"maximum is less than or equal to 100\",\n",
      "        \"firstField\": \"max\",\n",
      "        \"value\": 100.0,\n",
      "        \"op\": \"LE\",\n",
      "        \"verbose\": false,\n",
      "        \"quantileValue\": 0.0\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the specific types of constraints\n",
    "# the ranges of the between constraints include the bouding values\n",
    "\n",
    "# check if the maximum value of the column is in the range [5, 10.8]\n",
    "max_between_values = maxBetweenConstraint(lower_value=5, upper_value=10.8) \n",
    "# check if the maximum value of the column is less than or equal to 100\n",
    "max_less_than_equal_value = maxLessThanEqualConstraint(value=100)\n",
    "# check if the mean of the column is in the range [1.2, 1.6] \n",
    "mean_between_values = meanBetweenConstraint(lower_value=1.2, upper_value=1.6)\n",
    "# check if the minimum value of the column is in the range [0.1, 0.5]\n",
    "min_between_values = minBetweenConstraint(lower_value=0.1, upper_value=0.5)\n",
    "# check if the minimum value of the column is greater than or equal to 1\n",
    "min_greater_than_equal_value = minGreaterThanEqualConstraint(value=1)\n",
    "# check if the standard deviation of the column is in the range [2.3, 5.4]\n",
    "stddev_between_values = stddevBetweenConstraint(lower_value=2.3, upper_value=5.4)\n",
    "# check if the 0.15 quantile value is in the range [2, 4.3]\n",
    "quantile_between_values = quantileBetweenConstraint(quantile_value = 0.15, lower_value=2, upper_value=4.3) \n",
    "\n",
    "# example data frame with columns \"col1\",\"col2\", \"col3\"\n",
    "# you can also read an existing data set using pandas, or as a numpy array\n",
    "df = pd.DataFrame({\n",
    "    \"col1\": [4, 5, 6, 7],\n",
    "    \"col2\": [0, 1, 2, 3],\n",
    "    \"col3\": [50, 60, 80, 110]\n",
    "})\n",
    "\n",
    "# bind the standard deviation between constraint to the dataframe column named \"col1\"\n",
    "# bind the mean between constraint to the dataframe column named \"col2\"\n",
    "# you can add multiple summary constrants for each column\n",
    "dc = DatasetConstraints(None, summary_constraints={\n",
    "    \"col1\": [max_between_values, stddev_between_values, min_greater_than_equal_value], \n",
    "    \"col2\": [mean_between_values, min_between_values, quantile_between_values],\n",
    "    \"col3\": [max_less_than_equal_value]\n",
    "})  \n",
    "\n",
    "# logging the dataframe creates a profile with summary statistics for the data set\n",
    "# the data set profile contains column profiles with summary statistics for each column present in the data set\n",
    "profile = session.log_dataframe(df, \"test.data\", constraints=dc)\n",
    "\n",
    "# serialize the DatasetConstraints to JSON\n",
    "dc_json = json.loads(dc.to_json())\n",
    "col1_constraints = json.dumps(dc_json['summaryConstraints']['col1']['constraints'], indent=4)\n",
    "col2_constraints = json.dumps(dc_json['summaryConstraints']['col2']['constraints'], indent=4)\n",
    "col3_constraints = json.dumps(dc_json['summaryConstraints']['col3']['constraints'], indent=4)\n",
    "\n",
    "print(f\"Constraints for column 'col1': \\n{col1_constraints}\\n\")\n",
    "print(f\"Constraints for column 'col2': \\n{col2_constraints}\\n\")\n",
    "print(f\"Constraints for column 'col3': \\n{col3_constraints}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3cc64",
   "metadata": {},
   "source": [
    "#### Summary constraints are applied with apply_summary_constraints on the DatasetProfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5625be78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "col1:\n",
      "    test_name                                    total_run    failed\n",
      "    summary max BTWN 5 and 10.8                          1         0\n",
      "    standard deviation is between 2.3 and 5.4            1         1\n",
      "    minimum is greater than or equal to 1                1         0\n",
      "col2:\n",
      "    test_name                                      total_run    failed\n",
      "    mean is between 1.2 and 1.6                            1         0\n",
      "    minimum is between 0.1 and 0.5                         1         1\n",
      "    0.15-th quantile value is between 2 and 4.3            1         1\n",
      "col3:\n",
      "    test_name                               total_run    failed\n",
      "    maximum is less than or equal to 100            1         1\n"
     ]
    }
   ],
   "source": [
    "# summary constraints must be applied on the dataset profile, only after some data has been logged\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42655a1",
   "metadata": {},
   "source": [
    "As we can see **mean BTWN** passes and the **stddev BTWN** fails as they should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e5ee9",
   "metadata": {},
   "source": [
    "## Summary constraints for distinct, unique and most common values in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daf512",
   "metadata": {},
   "source": [
    "### Distinct values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e1e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import (\n",
    "    distinctValuesInSetConstraint, distinctValuesEqualSetConstraint, distinctValuesContainSetConstraint )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ae7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_set = distinctValuesInSetConstraint(reference_set=set(range(1, 10)))\n",
    "eq_set = distinctValuesEqualSetConstraint(reference_set={'a', 'a', 'a'})\n",
    "contain_set = distinctValuesContainSetConstraint(reference_set={0, 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151629fd",
   "metadata": {},
   "source": [
    "#### Applying summary constraints sent as an argument to apply_summary_constraints function on the same profile as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5370632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "col1:\n",
      "    test_name                                             total_run    failed\n",
      "    distinct values are in {1, 2, 3, 4, 5, 6, 7, 8, 9}            1         0\n",
      "    distinct values are equal to the set {'a'}                    1         1\n",
      "col2:\n",
      "    test_name                                 total_run    failed\n",
      "    distinct values contain the set {0, 1}            1         0\n"
     ]
    }
   ],
   "source": [
    "report = profile.apply_summary_constraints({'col1': SummaryConstraints([in_set, eq_set]), \n",
    "                                           'col2': SummaryConstraints([contain_set])})\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdd65c",
   "metadata": {},
   "source": [
    "### Unique column value count and proportion constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "221efa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import (\n",
    "    columnUniqueValueCountBetweenConstraint,\n",
    "    columnUniqueValueProportionBetweenConstraint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b04634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data set with customers, the country they live in, and their spending\n",
    "customer_data = pd.DataFrame({\n",
    "    \"customer\": [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\"],\n",
    "    \"country\": [\"Germany\", \"Italy\", \"Germany\", \"USA\", \"Germany\", \"UK\"],\n",
    "    \"spending\": [1200, 500, 700, 1500, 300, None]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc144d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "country:\n",
      "    test_name                                              total_run    failed\n",
      "    number of unique values is between 1 and 5                     1         0\n",
      "    proportion of unique values is between 0.3 and 0.45            1         1\n"
     ]
    }
   ],
   "source": [
    "# check if there are between 1 and 5 unique values in the specific column\n",
    "unique_value_count_between = columnUniqueValueCountBetweenConstraint(lower_value=1, upper_value=5)\n",
    "# check if the proportion of unique values in the set is between 0.3 and 0.4 inclusive\n",
    "unique_value_proportion_between = columnUniqueValueProportionBetweenConstraint(lower_fraction=0.3, upper_fraction=0.45)\n",
    "dc = DatasetConstraints(None, summary_constraints={\"country\": [unique_value_count_between, unique_value_proportion_between]})\n",
    "\n",
    "# log the customer_data dataframe to obtain the profile\n",
    "profile = session.log_dataframe(customer_data, 'test2.data', constraints=dc)\n",
    "# summary constraints must be applied on the profile after the data set has been logged\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bad55b",
   "metadata": {},
   "source": [
    "### Column most common value in set constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf56143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnMostCommonValueInSetConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6f9c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "country:\n",
      "    test_name                                       total_run    failed\n",
      "    most common value is in {'Germany', 'Italy'}            1         0\n"
     ]
    }
   ],
   "source": [
    "# check if the most common value in the column is in the set {\"Germany\", \"Italy\"}\n",
    "most_common_value_in_set = columnMostCommonValueInSetConstraint(value_set={\"Germany\", \"Italy\"})\n",
    "# bind the constraint to the column named \"country\"\n",
    "summary_constraint = {\"country\": [most_common_value_in_set]}\n",
    "# apply the summary constraints on the same profile for the customer_data data set\n",
    "report = profile.apply_summary_constraints(summary_constraint)\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67689b0",
   "metadata": {},
   "source": [
    "### Column values not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10453a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnValuesNotNullConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1cbc8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "customer:\n",
      "    test_name                          total_run    failed\n",
      "    does not contain missing values            1         0\n",
      "spending:\n",
      "    test_name                          total_run    failed\n",
      "    does not contain missing values            1         1\n"
     ]
    }
   ],
   "source": [
    "# check if all values in the column are non-null\n",
    "customer_value_not_null = columnValuesNotNullConstraint()\n",
    "spending_value_not_null = columnValuesNotNullConstraint()\n",
    "# bind the constraint to the column, there are no null values in the customer column, but there is one in the spending column\n",
    "summary_constraint = {\"customer\": [customer_value_not_null], \"spending\": [spending_value_not_null]}\n",
    "# apply the summary constraints on the same profile for the customer_data data set\n",
    "report = profile.apply_summary_constraints(summary_constraint)\n",
    "\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34623eb",
   "metadata": {},
   "source": [
    "### Missing values proportion constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c92718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import missingValuesProportionBetweenConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e912ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "customer:\n",
      "    test_name                                             total_run    failed\n",
      "    missing values proportion is between 0.0% and 1.0%            1         0\n",
      "spending:\n",
      "    test_name                                               total_run    failed\n",
      "    missing values proportion is between 0.0% and 17.0%             1         0\n",
      "    missing values proportion is between 10.0% and 15.0%            1         1\n"
     ]
    }
   ],
   "source": [
    "# check if the proportion of the missing values is between 0.0 % and 1.0 %\n",
    "customer_mvpbc = missingValuesProportionBetweenConstraint(lower_fraction=0.0, upper_fraction=0.01) # no missing values in the \"customer\" column so this constraint passes\n",
    "\n",
    "# check if the proportion of the missing values is between 0.0 % and 17.0 %\n",
    "spending_mvpbc = missingValuesProportionBetweenConstraint(lower_fraction=0.0, upper_fraction=0.17) # 1 of 6 missing values in the \"spending\" column, passes as well\n",
    "\n",
    "# check if the proportion of the missing values is between 0.0 % and 15.0 %\n",
    "spending_mvpbc2 = missingValuesProportionBetweenConstraint(lower_fraction=0.1, upper_fraction=0.15) # 1 of 6 is missing, resulting in a missing proportion of 16.6667 %, this one fails\n",
    "\n",
    "# bind the constraint to the column\n",
    "summary_constraint = {\"customer\": [customer_mvpbc], \"spending\": [spending_mvpbc, spending_mvpbc2]}\n",
    "# apply the summary constraints on the same profile for the customer_data data set\n",
    "report = profile.apply_summary_constraints(summary_constraint)\n",
    "\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85adee1",
   "metadata": {},
   "source": [
    "No missing values in the **\"customer\"** column so the constraint requiring missing proportion between 0% and 1% passes.\n",
    "\n",
    "Only 1 of 6 missing values in the **\"spending\"** column, the constraint requiring missing proportion between 0% and 17% passes as well\n",
    "\n",
    "This time 1 of 6 missing for the **\"spending\"** column is too much, resulting in a missing proportion of 16.6667%, but the constraint requires missing proportion of 10% to 15%, resulting in a fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f72c4",
   "metadata": {},
   "source": [
    "### Column value type equals or is in set constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30e2f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import (\n",
    "    columnValuesTypeEqualsConstraint,\n",
    "    columnValuesTypeInSetConstraint\n",
    ")\n",
    "from whylogs.proto import InferredType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a61d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "country:\n",
      "    test_name                              total_run    failed\n",
      "    type of the column values is STRING            1         0\n",
      "spending:\n",
      "    test_name                                                     total_run    failed\n",
      "    type of the column values is in {'FRACTIONAL', 'INTEGRAL'}            1         0\n"
     ]
    }
   ],
   "source": [
    "# check if the values of the specified column are of type string\n",
    "column_values_type_equals_string = columnValuesTypeEqualsConstraint(expected_type=InferredType.Type.STRING)\n",
    "# check if the values of the specified column are either fractional or integral numbers\n",
    "type_set = {InferredType.Type.FRACTIONAL, InferredType.Type.INTEGRAL}\n",
    "column_value_types_in_set = columnValuesTypeInSetConstraint(type_set=type_set, verbose=True)\n",
    "\n",
    "column_type_summary_constraint = {\n",
    "    \"country\": [column_values_type_equals_string],\n",
    "    \"spending\": [column_value_types_in_set]\n",
    "}\n",
    "\n",
    "# apply the summary constraints on the same profile for the customer_data data set\n",
    "report = profile.apply_summary_constraints(column_type_summary_constraint)\n",
    "# should not have failures since the country column type is string, and the spending column contains numbers\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaaa2dd",
   "metadata": {},
   "source": [
    "# Column values in set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c669679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnValuesInSetConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7acd883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "grade:\n",
      "    test_name                                  total_run    failed\n",
      "    values are in {'A', 'F', 'B', 'C', 'E'}            5         1\n"
     ]
    }
   ],
   "source": [
    "student_grades = pd.DataFrame({\n",
    "    'student_id': [1, 5, 15, 16, 22],\n",
    "    'grade': ['C', 'C', 'A', '/', 'B']\n",
    "})\n",
    "\n",
    "val_set = {'A', 'B', 'C', 'E', 'F'}  # valid grades\n",
    "column_values_in_set = columnValuesInSetConstraint(value_set=val_set)\n",
    "\n",
    "dc = DatasetConstraints(None, value_constraints={\n",
    "    \"grade\": [column_values_in_set], \n",
    "})\n",
    "\n",
    "# the value constraints are applied at the time of logging the dataframe\n",
    "profile = session.log_dataframe(student_grades, \"test.data\", constraints=dc)\n",
    "\n",
    "# out of the five student's grades we expect to see one failure for the '/' unknown grade\n",
    "# the total number of runs of the constraint should equal the number of values in the column\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cb3aa",
   "metadata": {},
   "source": [
    "# Regex matching constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661890e",
   "metadata": {},
   "source": [
    "### String length value constraints using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea7d2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "str1:\n",
      "    test_name                                          total_run    failed\n",
      "    length of the string values is equal to 7                  7         5\n",
      "    length of the string values is between 7 and 10            7         2\n"
     ]
    }
   ],
   "source": [
    "from whylogs.core.statistics.constraints import stringLengthEqualConstraint, stringLengthBetweenConstraint\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\"str1\": \"length7\"},\n",
    "        {\"str1\": \"length_8\"},\n",
    "        {\"str1\": \"length__9\"},\n",
    "        {\"str1\": \"a       10\"},\n",
    "        {\"str1\": \"11        b\"},\n",
    "        {\"str1\": '(*&^%^&*(24!@_+>:|}?><\"\\\\'},\n",
    "        {\"str1\": \"1b34567\"},\n",
    "    ]\n",
    ")\n",
    "length_constraint7 = stringLengthEqualConstraint(length=7)\n",
    "length_constraint7to10 = stringLengthBetweenConstraint(lower_value=7, upper_value=10)\n",
    "length_constraints = [length_constraint7, length_constraint7to10]\n",
    "dc = DatasetConstraints(None, value_constraints={\"str1\": length_constraints})\n",
    "\n",
    "profile = session.log_dataframe(df, 'test2.data', constraints=dc)\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a455e",
   "metadata": {},
   "source": [
    "### Email matching constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b679674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import containsEmailConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "041bd248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "email:\n",
      "    test_name                                      total_run    failed\n",
      "    column values match the email regex pattern            8         4\n"
     ]
    }
   ],
   "source": [
    "customer_emails = pd.DataFrame([\n",
    "    {\"email\": r\"abc's@gmail.com\"},  # valid\n",
    "    {\"email\": r'\"aVrrR Test \\@\"@gmail.com'},  # valid (if wrapped in quotes, emails can contain special characters)\n",
    "    {\"email\": r\"abc..q12@example.us\"},  # invalid (two consecutive dots)\n",
    "    {\"email\": r'\"sdsss\\d\"@gmail.com'},  # valid\n",
    "    {\"email\": r\"customer/department=shipping?@example-another.some-other.us\"},  # valid\n",
    "    {\"email\": r\".should_fail@yahoo.com\"},  # invalid (must not start wiht dot)\n",
    "    {\"email\": r\"some.@a.com\"},  # invalid (must not contain a dot directly before the @ symbol)\n",
    "    {\"email\": r\"abs@yahoo.\"},  # invalid (must not end with a dot)\n",
    "])\n",
    "\n",
    "# use the predefined email regex from whylogs\n",
    "default_contains_email_constraint = containsEmailConstraint()\n",
    "\n",
    "dc = DatasetConstraints(None, value_constraints={\"email\": [default_contains_email_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(customer_emails, 'test.data', constraints=dc)\n",
    "# we expect 4 of the 8 runs to be failures\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4f02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: supplying your own regex pattern might cause slower evaluation of the containsEmailConstraint, depending on its complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "email:\n",
      "    test_name                                      total_run    failed\n",
      "    column values match the email regex pattern            8         1\n"
     ]
    }
   ],
   "source": [
    "# you can provide your own email regex and check the values against it\n",
    "custom_contains_email_constraint = containsEmailConstraint(regex_pattern = r\"\\S+@\\S+\")\n",
    "dc = DatasetConstraints(None, value_constraints={\"email\": [custom_contains_email_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(customer_emails, 'test.data', constraints=dc)\n",
    "# now we expect 1 of the 8 runs to be failures, the email that contains white spaces\n",
    "format_report(dc.report())\n",
    "# running the containsEmailConstraint with your own regex pattern may cause slow evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959c36e",
   "metadata": {},
   "source": [
    "### Credit Card matching constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73901092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import containsCreditCardConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0d7e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "credit_card:\n",
      "    test_name                                            total_run    failed\n",
      "    column values match the credit card regex pattern           19         5\n"
     ]
    }
   ],
   "source": [
    "credit_cards = pd.DataFrame(\n",
    "    [\n",
    "        {\"credit_card\": \"3714-496353-98431\"},  # amex\n",
    "        {\"credit_card\": \"3787 344936 71000\"},  # amex\n",
    "        {\"credit_card\": \"3056 930902 5904\"},  # diners club\n",
    "        {\"credit_card\": \"3065 133242 2899\"},  # invalid\n",
    "        {\"credit_card\": \"3852-000002-3237\"},  # diners club\n",
    "        {\"credit_card\": \"6011 1111 1111 1117\"},  # discover\n",
    "        {\"credit_card\": \"6011-0009-9013-9424\"},  # discover\n",
    "        {\"credit_card\": \"3530 1113 3330 0000\"},  # jcb\n",
    "        {\"credit_card\": \"3566-0020-2036-0505\"},  # jcb\n",
    "        {\"credit_card\": \"5555 5555 5555 4444\"},  # master card\n",
    "        {\"credit_card\": \"5105 1051 0510 5100\"},  # master card\n",
    "        {\"credit_card\": \"4111 1111 1111 1111\"},  # visa\n",
    "        {\"credit_card\": \"4012 8888 8888 1881\"},  # visa\n",
    "        {\"credit_card\": \"4222-2222-2222-2222\"},  # visa\n",
    "        {\"credit_card\": \"1111-1111-1111-1111\"},  # invalid\n",
    "        {\"credit_card\": \"a4111 1111 1111 1111b\"},  # invalid\n",
    "        {\"credit_card\": \"4111111111111111\"},  # visa\n",
    "        {\"credit_card\": 12345},  # invalid\n",
    "        {\"credit_card\": \"absfcvs\"},  # invalid\n",
    "    ]\n",
    ")\n",
    "\n",
    "default_credit_card_constraint = containsCreditCardConstraint()\n",
    "dc = DatasetConstraints(None, value_constraints={\"credit_card\": [default_credit_card_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(credit_cards, 'test.data', constraints=dc)\n",
    "# now we expect 5 of the 19 runs to be failures, the invalid credit cards\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ce86172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: supplying your own regex pattern might cause slower evaluation of the containsCreditCardConstraint, depending on its complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "credit_card:\n",
      "    test_name                                            total_run    failed\n",
      "    column values match the credit card regex pattern           19         8\n"
     ]
    }
   ],
   "source": [
    "# you can provide your own credit card regex and check the values against it\n",
    "custom_credit_card_constraint = containsCreditCardConstraint(regex_pattern = r\"^(?:[0-9]{4}[\\s-]?){3,4}$\")\n",
    "dc = DatasetConstraints(None, value_constraints={\"credit_card\": [custom_credit_card_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(credit_cards, 'test.data', constraints=dc)\n",
    "# now more valid credit cards are being reported as failures\n",
    "format_report(dc.report())\n",
    "# running the containsCreditCardConstraint with your own regex pattern may cause slow evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15354468",
   "metadata": {},
   "source": [
    "### SSN regex matching constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e475de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import containsSSNConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d956856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "ssn:\n",
      "    test_name                                    total_run    failed\n",
      "    column values match the SSN regex pattern            8         4\n"
     ]
    }
   ],
   "source": [
    "ssn_data = pd.DataFrame([\n",
    "    {\"ssn\": \"123-01-2335\"},  # valid\n",
    "    {\"ssn\": \"039780012\"},  # valid\n",
    "    {\"ssn\": \"000231324\"},  # invalid\n",
    "    {\"ssn\": \"666781132\"},  # invalid\n",
    "    {\"ssn\": \"926-89-1234\"},  # invalid\n",
    "    {\"ssn\": \"001-01-0001\"},  # valid\n",
    "    {\"ssn\": \"122 23 0001\"},  # valid\n",
    "    {\"ssn\": \"1234-12-123\"},  # invalid\n",
    "])\n",
    "\n",
    "default_ssn_constraint = containsSSNConstraint()\n",
    "\n",
    "dc = DatasetConstraints(None, value_constraints={\"ssn\": [default_ssn_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(ssn_data, 'test.data', constraints=dc)\n",
    "# now we expect 4 of the 8 runs to be failures, the invalid ssn numbers\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7cf8fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: supplying your own regex pattern might cause slower evaluation of the containsSSNConstraint, depending on its complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "ssn:\n",
      "    test_name                                    total_run    failed\n",
      "    column values match the SSN regex pattern            8         5\n"
     ]
    }
   ],
   "source": [
    "# you can provide your own ssn regex and check the values against it\n",
    "custom_ssn_constraint = containsSSNConstraint(regex_pattern = r\"^[0-9]{3}-[0-9]{2}-[0-9]{4}$\")\n",
    "dc = DatasetConstraints(None, value_constraints={\"ssn\": [custom_ssn_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(ssn_data, 'test.data', constraints=dc)\n",
    "# now more valid ssn numbers are being reported as failures\n",
    "format_report(dc.report())\n",
    "# running the containsSSNConstraint with your own regex pattern may cause slow evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514241b0",
   "metadata": {},
   "source": [
    "### URL regex matching constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b6b8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import containsURLConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52460643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "url:\n",
      "    test_name                                    total_run    failed\n",
      "    column values match the URL regex pattern           10         4\n"
     ]
    }
   ],
   "source": [
    "web_urls = pd.DataFrame([\n",
    "    {\"url\": \"http://www.example.com\"},  # valid\n",
    "    {\"url\": \"abc.test.com\"},  # valid (without protocol)\n",
    "    {\"url\": \"abc.w23w.asb#abc?a=2\"},  # valid (without protocol)\n",
    "    {\"url\": \"https://ab.abc.bc\"},  # valid\n",
    "    {\"url\": \"a.b.c\"},  # valid\n",
    "    {\"url\": \"abcd\"},  # invalid\n",
    "    {\"url\": \"123.w23.235\"},  # valid\n",
    "    {\"url\": \"asf://saf.we.12\"},  # invalid\n",
    "    {\"url\": \"12345\"},  # invalid\n",
    "    {\"url\": \"1.2\"},  # invalid\n",
    "        \n",
    "])\n",
    "\n",
    "default_url_constraint = containsURLConstraint()\n",
    "dc = DatasetConstraints(None, value_constraints={\"url\": [default_url_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(web_urls, 'test.data', constraints=dc)\n",
    "# now we expect the 4 invalid urls, out of the 10 in total, to be reported as failures\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1bfc094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: supplying your own regex pattern might cause slower evaluation of the containsURLConstraint, depending on its complexity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "url:\n",
      "    test_name                                    total_run    failed\n",
      "    column values match the URL regex pattern           10         8\n"
     ]
    }
   ],
   "source": [
    "# you can provide your own url regex and check the values against it\n",
    "custom_url_constraint = containsURLConstraint(regex_pattern = r\"^http(s)?:\\/\\/(www\\.)?.+\\..+$\")\n",
    "dc = DatasetConstraints(None, value_constraints={\"url\": [custom_url_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(web_urls, 'test.data', constraints=dc)\n",
    "# with the new regex more valid urls are being reported as failures\n",
    "format_report(dc.report())\n",
    "# running the containsURLConstraint with your own regex pattern may cause slow evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff98762",
   "metadata": {},
   "source": [
    "# Datetime/json constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9fdc2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "str1:\n",
      "    test_name                                                                                                                                                                 total_run    failed\n",
      "    column values are dateutil parseable                                                                                                                                             14         9\n",
      "    column values are JSON parseable                                                                                                                                                 14        12\n",
      "    column values match the provided JSON schema {'type': 'object', 'properties': {'name': {'type': 'string'}, 'years': {'type': 'integer'}}, 'required': ['name', 'abc']}           14        12\n",
      "    column values are strftime parseable                                                                                                                                             14        12\n"
     ]
    }
   ],
   "source": [
    "from whylogs.core.statistics.constraints import (\n",
    "    dateUtilParseableConstraint, jsonParseableConstraint, matchesJsonSchemaConstraint, strftimeFormatConstraint )\n",
    "df = pd.DataFrame(\n",
    "        [\n",
    "            {\"str1\": \"1990-12-1\"},  # dateutil valid; strftime valid\n",
    "            {\"str1\": \"1990/12/1\"},\n",
    "            {\"str1\": \"today is 2019-03-27\"},  # dateutil invalid\n",
    "            {\"str1\": \"Monday at 12:01am\"},\n",
    "            {\"str1\": \"xyz_not_a_date\"},  # dateutil invalid\n",
    "            {\"str1\": \"yesterday\"},  # dateutil invalid\n",
    "            {\"str1\": {\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": 232, \"abc\": 1}},  # schema valid\n",
    "            {\"str1\": {\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": 232}},  # schema invalid\n",
    "            {\"str1\": json.dumps({\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": 232, \"abc\": 1})},  # json valid, schema valid\n",
    "            {\"str1\": json.dumps({\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": \"232\", \"abc\": 1})},  # json valid\n",
    "            {\"str1\": \"random str : fail everything\"},\n",
    "            {\"str1\": \"2003-12-23\"},  # strftime valid, dateutil valid\n",
    "            {\"str1\": \"2003-15-23\"},  # strftime invalid, dateutil invalid\n",
    "            {\"str1\": \"10-12-32\"},  # strftime invalid, dateutil valid\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dateutil_parseable = dateUtilParseableConstraint()\n",
    "json_parseable = jsonParseableConstraint()\n",
    "\n",
    "json_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"},\n",
    "            \"years\": {\"type\": \"integer\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"abc\"],\n",
    "    }\n",
    "matches_json_schema = matchesJsonSchemaConstraint(json_schema=json_schema)\n",
    "\n",
    "is_strftime = strftimeFormatConstraint(format=\"%Y-%m-%d\")\n",
    "\n",
    "apply_func_constraints = [dateutil_parseable, json_parseable, matches_json_schema, is_strftime]\n",
    "\n",
    "\n",
    "dc = DatasetConstraints(None, value_constraints={\"str1\": apply_func_constraints})\n",
    "profile = session.log_dataframe(df, 'test3.data', constraints=dc)\n",
    "\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af2279",
   "metadata": {},
   "source": [
    "Seeing the comments above, when creating the dataset, we can realize which values fail or pass, for which constraint. The dateutil constraint has 5 passing values in the dataset, and the other 3 constraints have only 2 values that pass from total of 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89c8d7",
   "metadata": {},
   "source": [
    "# Entropy and Distributional Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31901ff",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426903d3",
   "metadata": {},
   "source": [
    "Check if the column entropy is in some interval [a, b]. Works both for discrete and continuous valued columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a879dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import approximateEntropyBetweenConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d102965",
   "metadata": {},
   "source": [
    "#### Entropy on categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daa642d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = np.random.choice(['cat', 'dog', 'rabbit', 'hamster'], size=50, replace=True, p=[0.3, 0.1, 0.2, 0.4])\n",
    "pet_df = pd.DataFrame({\n",
    "    \"pet\": pets\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4cb1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "pet:\n",
      "    test_name                                     total_run    failed\n",
      "    approximate entropy is between 0.7 and 2.1            1         0\n"
     ]
    }
   ],
   "source": [
    "# check if the entropy of the pet_df 'pet' column is between 0.7 and 2.1 (the actual value is around 1.85)\n",
    "entropy_between_values_constraint = approximateEntropyBetweenConstraint(lower_value=0.7, upper_value=2.1)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"pet\": [entropy_between_values_constraint]})\n",
    "\n",
    "profile = session.log_dataframe(pet_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to complete without failures\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8171ca",
   "metadata": {},
   "source": [
    "#### Entropy on continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55e59691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 100 data points from normal distribution with mean 30000 and standard deviation 15000 to represent sales values\n",
    "sales = np.random.normal(loc=30000, scale=15000, size=100)\n",
    "\n",
    "sales_df = pd.DataFrame({\n",
    "    \"sales\": sales\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "931a2585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "sales:\n",
      "    test_name                                     total_run    failed\n",
      "    approximate entropy is between 2.3 and 3.5            1         1\n"
     ]
    }
   ],
   "source": [
    "# check if the entropy of the sales_df 'sales' column is between 2.3 and 3.5 (the actual value is 1.85)\n",
    "entropy_between_values_constraint_cont = approximateEntropyBetweenConstraint(lower_value=2.3, upper_value=3.5)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"sales\": [entropy_between_values_constraint_cont]})\n",
    "\n",
    "profile = session.log_dataframe(sales_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to fail since entropy is between 3.8 and 3.9\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e9ae2",
   "metadata": {},
   "source": [
    "### KS Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4dcd2",
   "metadata": {},
   "source": [
    "The KS Test can only be executed on continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c53fdd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import parametrizedKSTestPValueGreaterThanConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48251fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would be the reference distribution, sales 2020\n",
    "sales_2020 = np.random.normal(loc=30000, scale=15000, size=100)\n",
    "# this would be the target distribution, sales 2021\n",
    "sales_2021 = np.random.normal(loc=45000, scale=10000, size=100)\n",
    "# we want to check if the sales in 2020 have the same distribution as the sales in 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d265980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "sales:\n",
      "    test_name                                            total_run    failed\n",
      "    parametrized KS test p-value is greater than 0.05            1         1\n"
     ]
    }
   ],
   "source": [
    "sales_2021_df = pd.DataFrame({\n",
    "    \"sales\": sales_2021\n",
    "})\n",
    "\n",
    "# check if the p-value of the ks test for referenece distribution sales_2020 is greater than 0.05 \n",
    "# if so, we do not reject the null hypothesis\n",
    "ks_test_p_value_greater_than = parametrizedKSTestPValueGreaterThanConstraint(reference_distribution=sales_2020, p_value=0.05)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"sales\": [ks_test_p_value_greater_than]})\n",
    "\n",
    "profile = session.log_dataframe(sales_2021_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to fail since entropy is between 3.8 and 3.9\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958febe",
   "metadata": {},
   "source": [
    "The p-value is less than 0.05, which means we can reject the null hypothesis with this confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff32245",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9344e",
   "metadata": {},
   "source": [
    "The KL Divergence constraint is supported for both discrete and continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c729bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnKLDivergenceLessThanConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300513e",
   "metadata": {},
   "source": [
    "#### KL Divergence for continuous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d865e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "sales:\n",
      "    test_name                         total_run    failed\n",
      "    KL Divergence is less than 0.6            1         1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petar/whylogs/src/whylogs/core/summaryconverters.py:330: RuntimeWarning: divide by zero encountered in log\n",
      "  kl_divergence = np.sum(np.where(pmf_target != 0, pmf_target * np.log(pmf_target / pmf_reference), 0))\n",
      "/home/petar/whylogs/src/whylogs/core/summaryconverters.py:330: RuntimeWarning: invalid value encountered in multiply\n",
      "  kl_divergence = np.sum(np.where(pmf_target != 0, pmf_target * np.log(pmf_target / pmf_reference), 0))\n"
     ]
    }
   ],
   "source": [
    "# check if the kl divergence is greater than 0.6 \n",
    "kl_divergence_greater_than = columnKLDivergenceLessThanConstraint(reference_distribution=sales_2020, threshold=0.6)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"sales\": [kl_divergence_greater_than]})\n",
    "\n",
    "profile = session.log_dataframe(sales_2021_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to fail\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba531d",
   "metadata": {},
   "source": [
    "The distribution of sales in 2020 cannot be encoded with the distribution of sales in 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0d8f1",
   "metadata": {},
   "source": [
    "#### KL Divergence for discrete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e22e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "pet:\n",
      "    test_name                         total_run    failed\n",
      "    KL Divergence is less than 0.6            1         0\n"
     ]
    }
   ],
   "source": [
    "# create a new distribtution from the pets sample with different probabilities\n",
    "pets_reference = np.random.choice(['cat', 'dog', 'rabbit', 'hamster'], size=50, replace=True, p=[0.5, 0.1, 0.2, 0.2])\n",
    "\n",
    "# check if the kl divergence is greater than 0.6 \n",
    "kl_divergence_greater_than = columnKLDivergenceLessThanConstraint(reference_distribution=pets_reference, threshold=0.6)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"pet\": [kl_divergence_greater_than]})\n",
    "\n",
    "profile = session.log_dataframe(pet_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to pass\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3abc84",
   "metadata": {},
   "source": [
    "### Chi-Squared Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c2790",
   "metadata": {},
   "source": [
    "The Chi-Squared test constraint is only supported for **categorical** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efa2de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnChiSquaredTestPValueGreaterThanConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbde8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "pet:\n",
      "    test_name                                        total_run    failed\n",
      "    Chi-Squared test p-value is greater than 0.05            1         1\n"
     ]
    }
   ],
   "source": [
    "# create a new distribtution from the pets sample with different probabilities\n",
    "pets_reference = np.random.choice(['cat', 'dog', 'rabbit', 'hamster'], size=50, replace=True, p=[0.01, 0.01, 0.97, 0.01])\n",
    "\n",
    "# check if the p-value is greater than 0.05\n",
    "chi_squared_p_value_greater_than = columnChiSquaredTestPValueGreaterThanConstraint(reference_distribution=pets_reference, p_value=0.05)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"pet\": [chi_squared_p_value_greater_than]})\n",
    "\n",
    "profile = session.log_dataframe(pet_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to fail since the distributions are different\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa9046",
   "metadata": {},
   "source": [
    "The p-value is not greater than 0.05, which means that we can reject the null hypothesis that the distributions are equal within this confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b589597",
   "metadata": {},
   "source": [
    "If you don't have a reference distribution for calculating the Chi-Squared test, but you know the approximate frequencies of each of the items, you can use this constraint by supplying a mapping of items and frequencies as counts, in the **reference_distribution** parameter of the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4f105f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint failures by feature - \n",
      "pet:\n",
      "    test_name                                        total_run    failed\n",
      "    Chi-Squared test p-value is greater than 0.05            1         1\n"
     ]
    }
   ],
   "source": [
    "# create a new distribtution from the pets sample with different probabilities\n",
    "reference_dict_pets = {\n",
    "    'cat': 1,\n",
    "    'dog': 1,\n",
    "    'rabbit': 48, \n",
    "    'hamster': 1,\n",
    "}\n",
    "\n",
    "# check if the p_value is greater than 0.05\n",
    "chi_squared_p_value_greater_than = columnChiSquaredTestPValueGreaterThanConstraint(reference_distribution=reference_dict_pets, p_value=0.05)\n",
    "\n",
    "dc = DatasetConstraints(None, summary_constraints={\"pet\": [chi_squared_p_value_greater_than]})\n",
    "\n",
    "profile = session.log_dataframe(pet_df, 'test.data', constraints=dc)\n",
    "# now we expect the constraint to fail since this is approximately the same distribution from the previous example\n",
    "report = profile.apply_summary_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78607967",
   "metadata": {},
   "source": [
    "The p-value is not greater than 0.05, which means that we can reject the null hypothesis that the distributions are equal within this confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4113311",
   "metadata": {},
   "source": [
    "## Table shape constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cceadc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import (\n",
    "    numberOfRowsConstraint, columnExistsConstraint, columnsMatchSetConstraint )\n",
    "\n",
    "df = pd.DataFrame(\n",
    "        [\n",
    "            {\"str1\": \"random1\"},\n",
    "            {\"str1\": \"random2\"},\n",
    "            {\"str1\": \"random 4-1\"},\n",
    "            {\"str1\": \"4 random\"},\n",
    "            {\"str1\": \"whylogs rocks!\"},\n",
    "            {\"str1\": \"   \"},\n",
    "            {\"str1\": 12},\n",
    "            {\"str1\": {\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": 232}},\n",
    "            {\"str1\": json.dumps({\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": 232, \"abc\": 1})},\n",
    "            {\"str1\": json.dumps({\"name\": \"s\", \"w2w2\": \"dgsg\", \"years\": \"232\", \"abc\": 1})},\n",
    "            {\"str1\": \"random str : fail everything\"},\n",
    "            {\"str1\": \"2003-12-23\"},\n",
    "            {\"str1\": \"2003-15-23\"},\n",
    "            {\"str1\": \"10-12-32\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df['col2'] = range(len(df))\n",
    "\n",
    "rows = numberOfRowsConstraint(n_rows=len(df)+1) # fail\n",
    "rows_2 = numberOfRowsConstraint(n_rows=len(df)) # pass\n",
    "\n",
    "column_exist = columnExistsConstraint(\"this_column_does_not_exist\") # fail\n",
    "column_exist2 = columnExistsConstraint(\"col2\") # pass\n",
    "\n",
    "set1 = {'this', 'is', 'a', 'wrong', 'columns', 'set'}\n",
    "columns_set = set(df.columns)\n",
    "columns_match = columnsMatchSetConstraint(set1) # fail\n",
    "columns_match2 = columnsMatchSetConstraint(columns_set) # pass\n",
    "\n",
    "table_shape_constraints = [rows, rows_2, column_exist, column_exist2, columns_match, columns_match2]\n",
    "\n",
    "dc = DatasetConstraints(None, table_shape_constraints=table_shape_constraints)\n",
    "\n",
    "profile = session.log_dataframe(df, \"test.data\", constraints=dc)\n",
    "\n",
    "report = profile.apply_table_shape_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44652a",
   "metadata": {},
   "source": [
    "### Table shape example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "339bdec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = session.logger(dataset_name=\"test2.data\", constraints=dc)\n",
    "logger.log_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56629731",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = logger.profile.apply_table_shape_constraints()\n",
    "format_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f56f39",
   "metadata": {},
   "source": [
    "Logging another dataframe with different DatasetProfile but the same DatasetConstraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f510f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log({\"this_column_does_not_exist\": 1})  # logging a new non existent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63280b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = logger.profile.apply_table_shape_constraints()\n",
    "format_report(report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78904486",
   "metadata": {},
   "source": [
    "After logging the column **'this_column_does_not_exist'**, the total row number stays the same, \n",
    "so the numberOfRowsConstraint passed.\n",
    "\n",
    "**'table columns CONTAIN this_column_does_not_exist'** constraint now passed, since the column now exists, but\n",
    "\n",
    "**'table columns EQ {'str1', 'col2'}'** now failed, because new column was logged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2c41e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "set2 = set(columns_set)\n",
    "set2.add(\"this_column_does_not_exist\")\n",
    "\n",
    "columns_match3 = columnsMatchSetConstraint(set2) # new constraint containing the new column\n",
    "\n",
    "report3 = logger.profile.apply_table_shape_constraints(SummaryConstraints([columns_match3])) # applying only the new constraint\n",
    "format_report(report3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f346c",
   "metadata": {},
   "source": [
    "After adding the new column to **'set2'** and creating a **columnsMatchSetConstraint** with it, now it doesn't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "688f2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = dict()\n",
    "    # logging a new value for every column (one more row)\n",
    "for column in df.columns:\n",
    "    value = df[column][10]  # sample from the column\n",
    "    log_dict[column] = value\n",
    "logger.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "668c93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report4 = logger.profile.apply_table_shape_constraints()\n",
    "format_report(report4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3f878",
   "metadata": {},
   "source": [
    "**'table total_row_number EQ 14'** now failed since new row was logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0ba26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_3 = numberOfRowsConstraint(n_rows=len(df.index) + 1)  # new numberOfRowsConstraint\n",
    "report5 = logger.profile.apply_table_shape_constraints(SummaryConstraints([rows_3]))\n",
    "format_report(report5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a12eb",
   "metadata": {},
   "source": [
    "Creating a new **numberOfRowsConstraint** with n_rows = previous_n_rows + 1 and applying it, now does not fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6058796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "profile = logger.close()  # closing the logger and getting the DatasetProfile\n",
    "print (profile.total_row_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956cf2b",
   "metadata": {},
   "source": [
    "## Multi column constraints\n",
    "### Logical operations between values of the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ae20c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnValuesAGreaterThanBConstraint, columnValuesAEqualBConstraint\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [4, 5, 6, 7], \"col2\": [0, 1, 6, 15]})\n",
    "\n",
    "a_gt_b = columnValuesAGreaterThanBConstraint(column_A=\"col1\", column_B=\"col2\")\n",
    "a_eq_b = columnValuesAEqualBConstraint(column_A=\"col1\", column_B=\"col2\")\n",
    "\n",
    "dc = DatasetConstraints(None, multi_column_value_constraints=[a_gt_b, a_eq_b])\n",
    "\n",
    "profile = session.log_dataframe(df, \"test4.data\", constraints=dc)\n",
    "\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76062557",
   "metadata": {},
   "source": [
    "Value by value comparison. col1 values > col2 values, only 2 are passing, and col1 values == col 2 values only 1 is True (the third element from both the columns are equal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744614c",
   "metadata": {},
   "source": [
    "### Sum of row values of multiple columns equals some value, or some column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae986496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import sumOfRowValuesOfMultipleColumnsEqualsConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5a8a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expences = pd.DataFrame({\n",
    "    \"employees %\": [25, 45, 15, 3],\n",
    "    \"equipment %\": [10, 12, 4, 9],\n",
    "    \"materials %\": [40, 35, 45, 55],\n",
    "    \"other %\": [25, 8, 4, 6]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62430c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the percentage of expenses for each part sum to 100 %\n",
    "sum_of_row_values_eq_100 = sumOfRowValuesOfMultipleColumnsEqualsConstraint(\n",
    "    columns=[\"employees %\", \"equipment %\", \"materials %\", \"other %\"],\n",
    "    value=100\n",
    ")\n",
    "\n",
    "dc = DatasetConstraints(None, multi_column_value_constraints=[sum_of_row_values_eq_100])\n",
    "\n",
    "# the multicolumn value constraints do not need to be applied to the data \n",
    "# they are applied at the time of logging\n",
    "profile = session.log_dataframe(total_expences, \"test.data\", constraints=dc)\n",
    "\n",
    "# we expect 2 of the 4 rows to be failures since the last two rows do not sum to 100\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d1b7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sum of the row values (percentages) for 'equipment %' and 'materials %' equal the value of 'other %'\n",
    "sum_of_row_values_eq_100 = sumOfRowValuesOfMultipleColumnsEqualsConstraint(\n",
    "    columns=[\"equipment %\", \"materials %\"],\n",
    "    value='other %'\n",
    ")\n",
    "\n",
    "dc = DatasetConstraints(None, multi_column_value_constraints=[sum_of_row_values_eq_100])\n",
    "profile = session.log_dataframe(total_expences, \"test.data\", constraints=dc)\n",
    "\n",
    "# we expect all rows to be failures since the sum of 'equipment %' and 'materials %' is not equal to the value of the column 'other %'\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc748f",
   "metadata": {},
   "source": [
    "### Column Pair Values in Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa997a38",
   "metadata": {},
   "source": [
    "Check if the values of a pair of columns are in a predefined set of pair values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7cbd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnPairValuesInSetConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c679454",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_grades = pd.DataFrame({\n",
    "    \"product\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\", \"ProductE\"],\n",
    "    \"grade\": [\"A\", \"A\", \"B\", \"C\", \"C\"],\n",
    "    \"subgrade\": [\"A1\", \"A3\", \"B2\", \"C2\", \"C2\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6db3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to check if each of the grade and subgrade pairs are in the specific set\n",
    "grade_subgrade_pairs_in_set = columnPairValuesInSetConstraint(\n",
    "    column_A=\"grade\", \n",
    "    column_B=\"subgrade\",\n",
    "    value_set = {(\"A\", \"A1\"), (\"A\", \"A2\"), (\"B\", \"B1\"), (\"B\", \"B2\"), (\"C\", \"C1\"), (\"C\", \"C2\")}\n",
    ")\n",
    "\n",
    "dc = DatasetConstraints(None, multi_column_value_constraints=[grade_subgrade_pairs_in_set])\n",
    "profile = session.log_dataframe(product_grades, \"test.data\", constraints=dc)\n",
    "\n",
    "# we expect 1 out of 5 pairs to be a failure, specifically (\"A\", \"A3\")\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a21ad0",
   "metadata": {},
   "source": [
    "### Column Values Unique within Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52c403",
   "metadata": {},
   "source": [
    "Check if the value of the specified column is unique within each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42ef3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.core.statistics.constraints import columnValuesUniqueWithinRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db57da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.DataFrame({\n",
    "    \"first_name\": [\"John\", \"Jane\", \"Bob\", \"Anna\"],\n",
    "    \"last_name\": [\"Doe\", \"Doe\", \"Smith\", \"Jones\"],\n",
    "    \"username\": [\"jd123\", \"jane.doe@example.com\", \"bobsmith\", \"_anna_\"],\n",
    "    \"email\": [\"john.doe@example.com\", \"jane.doe@example.com\", \"bob.smith@example.com\", \"anna_jones@example.com\"],\n",
    "    \"followers\": [1525, 12268, 51343, 867],\n",
    "    \"points\": [23.4, 123.2, 432.22, 32.1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d78fc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the emails are unique compared to other fields for each user\n",
    "# suppose we do not want to accept a username which is the same as the user's email\n",
    "email_values_unique_within_row = columnValuesUniqueWithinRow(column_A=\"email\")\n",
    "\n",
    "dc = DatasetConstraints(None, multi_column_value_constraints=[email_values_unique_within_row])\n",
    "profile = session.log_dataframe(users, \"test.data\", constraints=dc)\n",
    "\n",
    "# we expect 1 out of 4 evaluations of the constraint to be a failure, since Jane Doe's email is the same as their username\n",
    "format_report(dc.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da9d24",
   "metadata": {},
   "source": [
    "# Generate default constraints for data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c6ede",
   "metadata": {},
   "source": [
    "Let's log the users data frame from the previous example, without any constraints. We will use WhyLogs' **generate_constraints** method to generate default constraints using the dataset profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b938730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = session.log_dataframe(users, \"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a552cdce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"properties\": {\n",
      "    \"schemaMajorVersion\": 1,\n",
      "    \"schemaMinorVersion\": 2,\n",
      "    \"sessionId\": \"6c9a7d7a-d50c-4e20-9be7-0757caf192cb\",\n",
      "    \"sessionTimestamp\": \"1645457691410\",\n",
      "    \"dataTimestamp\": \"1645454092967\",\n",
      "    \"tags\": {\n",
      "      \"name\": \"test.data\"\n",
      "    },\n",
      "    \"metadata\": {}\n",
      "  },\n",
      "  \"summaryConstraints\": {\n",
      "    \"email\": {\n",
      "      \"constraints\": [\n",
      "        {\n",
      "          \"name\": \"type of the column values is STRING\",\n",
      "          \"firstField\": \"column_values_type\",\n",
      "          \"value\": 5.0,\n",
      "          \"op\": \"EQ\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number of unique values is between 3 and 5\",\n",
      "          \"firstField\": \"unique_count\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": 3.0,\n",
      "            \"upperValue\": 5.0\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"most common value is in {'john.doe@example.com', 'bob.smith@example.com', 'jane.doe@example.com', 'anna_jones@example.com'}\",\n",
      "          \"firstField\": \"most_common_value\",\n",
      "          \"op\": \"IN\",\n",
      "          \"referenceSet\": [\n",
      "            \"john.doe@example.com\",\n",
      "            \"bob.smith@example.com\",\n",
      "            \"jane.doe@example.com\",\n",
      "            \"anna_jones@example.com\"\n",
      "          ],\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"points\": {\n",
      "      \"constraints\": [\n",
      "        {\n",
      "          \"name\": \"minimum is greater than or equal to 0\",\n",
      "          \"firstField\": \"min\",\n",
      "          \"value\": 0.0,\n",
      "          \"op\": \"GE\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"mean is between -38.98552432358383 and 344.44552432358387\",\n",
      "          \"firstField\": \"mean\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": -38.98552432358383,\n",
      "            \"upperValue\": 344.44552432358387\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"type of the column values is FRACTIONAL\",\n",
      "          \"firstField\": \"column_values_type\",\n",
      "          \"value\": 2.0,\n",
      "          \"op\": \"EQ\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"most common value is in {'432.22', '32.1', '123.2', '23.4'}\",\n",
      "          \"firstField\": \"most_common_value\",\n",
      "          \"op\": \"IN\",\n",
      "          \"referenceSet\": [\n",
      "            \"432.22\",\n",
      "            \"32.1\",\n",
      "            \"123.2\",\n",
      "            \"23.4\"\n",
      "          ],\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"username\": {\n",
      "      \"constraints\": [\n",
      "        {\n",
      "          \"name\": \"type of the column values is STRING\",\n",
      "          \"firstField\": \"column_values_type\",\n",
      "          \"value\": 5.0,\n",
      "          \"op\": \"EQ\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number of unique values is between 3 and 5\",\n",
      "          \"firstField\": \"unique_count\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": 3.0,\n",
      "            \"upperValue\": 5.0\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"most common value is in {'jd123', 'jane.doe@example.com', '_anna_', 'bobsmith'}\",\n",
      "          \"firstField\": \"most_common_value\",\n",
      "          \"op\": \"IN\",\n",
      "          \"referenceSet\": [\n",
      "            \"jd123\",\n",
      "            \"jane.doe@example.com\",\n",
      "            \"_anna_\",\n",
      "            \"bobsmith\"\n",
      "          ],\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"first_name\": {\n",
      "      \"constraints\": [\n",
      "        {\n",
      "          \"name\": \"type of the column values is STRING\",\n",
      "          \"firstField\": \"column_values_type\",\n",
      "          \"value\": 5.0,\n",
      "          \"op\": \"EQ\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number of unique values is between 3 and 5\",\n",
      "          \"firstField\": \"unique_count\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": 3.0,\n",
      "            \"upperValue\": 5.0\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"most common value is in {'Anna', 'Bob', 'Jane', 'John'}\",\n",
      "          \"firstField\": \"most_common_value\",\n",
      "          \"op\": \"IN\",\n",
      "          \"referenceSet\": [\n",
      "            \"Anna\",\n",
      "            \"Bob\",\n",
      "            \"Jane\",\n",
      "            \"John\"\n",
      "          ],\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"followers\": {\n",
      "      \"constraints\": [\n",
      "        {\n",
      "          \"name\": \"minimum is greater than or equal to 0\",\n",
      "          \"firstField\": \"min\",\n",
      "          \"value\": 0.0,\n",
      "          \"op\": \"GE\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"mean is between -7308.11238882488 and 40309.612388824884\",\n",
      "          \"firstField\": \"mean\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": -7308.11238882488,\n",
      "            \"upperValue\": 40309.612388824884\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"type of the column values is INTEGRAL\",\n",
      "          \"firstField\": \"column_values_type\",\n",
      "          \"value\": 3.0,\n",
      "          \"op\": \"EQ\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number of unique values is between 3 and 5\",\n",
      "          \"firstField\": \"unique_count\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": 3.0,\n",
      "            \"upperValue\": 5.0\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"most common value is in {'1525', '867', '51343', '12268'}\",\n",
      "          \"firstField\": \"most_common_value\",\n",
      "          \"op\": \"IN\",\n",
      "          \"referenceSet\": [\n",
      "            \"1525\",\n",
      "            \"867\",\n",
      "            \"51343\",\n",
      "            \"12268\"\n",
      "          ],\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"last_name\": {\n",
      "      \"constraints\": [\n",
      "        {\n",
      "          \"name\": \"type of the column values is STRING\",\n",
      "          \"firstField\": \"column_values_type\",\n",
      "          \"value\": 5.0,\n",
      "          \"op\": \"EQ\",\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number of unique values is between 2 and 4\",\n",
      "          \"firstField\": \"unique_count\",\n",
      "          \"op\": \"BTWN\",\n",
      "          \"between\": {\n",
      "            \"lowerValue\": 2.0,\n",
      "            \"upperValue\": 4.0\n",
      "          },\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"most common value is in {'Jones', 'Doe', 'Smith'}\",\n",
      "          \"firstField\": \"most_common_value\",\n",
      "          \"op\": \"IN\",\n",
      "          \"referenceSet\": [\n",
      "            \"Jones\",\n",
      "            \"Doe\",\n",
      "            \"Smith\"\n",
      "          ],\n",
      "          \"verbose\": false,\n",
      "          \"quantileValue\": 0.0\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"valueConstraints\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "auto_constraints = profile.generate_constraints()\n",
    "print(message_to_json(auto_constraints.to_protobuf()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea23ed",
   "metadata": {},
   "source": [
    "For the columns with inferred type STRING, the **generate_constraints** method generates 3 types of constraints: **columnValuesTypeEqualsConstraint** where the type is STRING, **columnUniqueValueCountBetweenConstraint** which makes a constraint that the unique values in a column should range between unique_count - 1 and unique_count + 1 in the current data frame, and finally **columnMostCommonValueInSetConstraint** which takes a set of the 5 most common values and defines a constraint that the most common value in this column should be in that set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f683579",
   "metadata": {},
   "source": [
    "The columns which have inferred type FRACTIONAL or INTEGRAL, such as 'points' and 'followers' respectively, have numeric constraints generated such as minimum value greater than 0, maximum value less than 0, mean in range [mean - stddev, mean + stddev], if these constraints apply to the current column. Apart from these constraints, **columnValuesTypeEqualsConstraint** and **columnMostCommonValueInSetConstraint** are generated for both types. **columnUniqueValueCountBetweenConstraint** is generated only for the INTEGRAL valued columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd56524",
   "metadata": {},
   "source": [
    "No constraints are generated for columns which have an inferred type of NULL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
