{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñ•Ô∏è Monitoring Classification Model Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll show how you can log performance metrics of your ML Model with whylogs, and how to send it to your dashboard at Whylabs Platform.\n",
    "We'll follow a classification use case, where we're trying to predict whether a given transaction will be cancelled, using data from [Retail Case Study Data](https://www.kaggle.com/darpan25bajaj/retail-case-study-data).\n",
    "\n",
    "We will:\n",
    "- Download Model/Features/Labels data from S3\n",
    "- Make predictions with the loaded models and features\n",
    "- Log Input/Output features with whylogs\n",
    "- Log Performance Metrics (Labels and Predictions) with whylogs\n",
    "- Show Performance summary at WhyLabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõçÔ∏è The Data Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we want to predict whether a given transaction will be cancelled, using data from a small retail business.\n",
    "\n",
    "\n",
    "Most applications receive labeled data at a substantial delay, often for a subset of the total data seen at inference time. That is why, in whylogs, ML model performance metrics can be treated separately from the input and output data that we see for data profiling. To illustrate that, we will download separate data on the actual values, predictions, and a threshold or probability score to determine performance metrics.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "The features used in this example contains information about:\n",
    "\n",
    "- Transaction\n",
    "    - Date of transaction\n",
    "    - Total amount\n",
    "    - Quantity\n",
    "- Product\n",
    "    - Product category\n",
    "    - Product subcategory\n",
    "- Customer\n",
    "    - Age\n",
    "    - Gender\n",
    "    - City code\n",
    "\n",
    "The dataset used is based on the original dataset present in [Retail Case Study Data](https://www.kaggle.com/darpan25bajaj/retail-case-study-data), with additional preprocessing and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install --upgrade pip -q\n",
    "pip install whylogs -U -q\n",
    "pip install sklearn -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the Artifacts from S3 (Model+Features+Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pickle\n",
    "import pandas as pd\n",
    "model_path = \"https://whylabs-public.s3.us-west-2.amazonaws.com/datasets/tour/perf/retail-rf-classifier.pickle\"\n",
    "features_path = \"https://whylabs-public.s3.us-west-2.amazonaws.com/datasets/tour/perf/transformed-current.csv\"\n",
    "labels_path = \"https://whylabs-public.s3.us-west-2.amazonaws.com/datasets/tour/perf/transformed-current-labels.csv\"\n",
    "\n",
    "\n",
    "model= pickle.load(urllib.request.urlopen(model_path))\n",
    "df = pd.read_csv(features_path)\n",
    "df_metrics = pd.read_csv(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.ensemble._forest.RandomForestClassifier"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the feature's names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Subcategory Code       int64\n",
       "Product Category Code          int64\n",
       "Quantity                       int64\n",
       "Item Price                   float64\n",
       "Total Tax                    float64\n",
       "Total Amount                 float64\n",
       "City Code                    float64\n",
       "Age at Transaction Date      float64\n",
       "Transaction Day of Week        int64\n",
       "Store Type.Flagship store      int64\n",
       "Store Type.MBR                 int64\n",
       "Store Type.TeleShop            int64\n",
       "Store Type.e-Shop              int64\n",
       "Gender.F                       int64\n",
       "Gender.M                       int64\n",
       "Gender.Unknown                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target field is `purchase canceled`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Purchase Canceled    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ Making the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the features and the model, we can use them to make the predictions. Later, when we log the metrics, we'll also need the prediction scores (to get ROC and Precision-Recall Curves). We trained our Random Forest Model with `SKLearn`. We have access to the scores using `predict_proba`, so let's call it, in addition to `predict`, to have the scores and the classes predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model.predict_proba(df)\n",
    "predict_class = model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úîÔ∏è Setup WhyLabs/Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will follow the same instructions as those you may find in the WhyLabs Observability Platform live data example instructions. In that workflow, you will gather your organization ID and API key if you haven't already and then upload a number of profiles new model.\n",
    "\n",
    "See detailed instructions in our documentation using the code in the cells below: https://docs.whylabs.ai/docs/whylabs-set-up-model\n",
    "\n",
    "Now we can add our API key and organization ID as environment variables and add a `WhyLabsWriter` to our whylogs session for automated upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your WhyLabs Org ID\n",
      "Enter your WhyLabs API key\n",
      "Using API Key ID:  xxtIbfnVKB\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "from whylogs.app import Session\n",
    "from whylogs.app.writers import WhyLabsWriter\n",
    "import getpass\n",
    "\n",
    "\n",
    "\n",
    "# set your org-id here\n",
    "print(\"Enter your WhyLabs Org ID\")\n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input()\n",
    "\n",
    "# set your API key here\n",
    "print(\"Enter your WhyLabs API key\")\n",
    "os.environ[\"WHYLABS_API_KEY\"] = getpass.getpass()\n",
    "print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])\n",
    "\n",
    "# Adding the WhyLabs Writer to utilize WhyLabs platform\n",
    "writer = WhyLabsWriter()\n",
    "session = Session(writers=[writer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Profiling Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first profile input data. \n",
    "\n",
    "Our dataframe contains transactions of one particular day. Let's log it as if it were for today.\n",
    "\n",
    "Remember to input `datasetID` to point to the right model. If it's your first model, that would be `model-1`, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using API key ID: xxtIbfnVKB\n"
     ]
    }
   ],
   "source": [
    "# Run whylogs on historical data and upload to WhyLabs.\n",
    "now = datetime.datetime.now()\n",
    "print(\"Enter your Dataset ID\")\n",
    "datasetID = input()\n",
    "with session.logger(\n",
    "    # Note: 'datasetId' in whylogs maps to 'model-id' that is provided when you set up a model in WhyLabs\n",
    "    tags={\"datasetId\": datasetID}, dataset_timestamp=now\n",
    ") as ylog:\n",
    "    ylog.log_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated earlier, we will need to log the prediction scores along with the actual prediction classes. That is needed to generate the ROC Curves Precision-recall curves. __SKLearn__'s `predict_proba` gives us the scores for each class. For example, in the code below, for the first prediction, the model yields a score of 0.89 for class 0 and 0.11 for class 1. Since the score for class 0 is higher than for class 1, the predicted class is `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89, 0.11],\n",
       "       [0.91, 0.09]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whylogs, we need to pass the score only for the predicted class. So let's create a `scores` list with only the highest score between the two classes, which is the predicted class' score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Purchase Canceled</th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Purchase Canceled  prediction  score\n",
       "0                  0.0         0.0   0.89\n",
       "1                  0.0         0.0   0.91\n",
       "2                  0.0         0.0   0.94\n",
       "3                  0.0         0.0   1.00\n",
       "4                  0.0         0.0   0.88\n",
       "..                 ...         ...    ...\n",
       "832                0.0         0.0   0.82\n",
       "833                0.0         0.0   0.93\n",
       "834                0.0         0.0   0.95\n",
       "835                1.0         0.0   0.98\n",
       "836                0.0         0.0   0.78\n",
       "\n",
       "[837 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics['prediction'] = predict_class\n",
    "\n",
    "scores = [max(p) for p in predict_proba]\n",
    "\n",
    "df_metrics['score'] = scores\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also cast our labels and predictions as integers, so WhyLabs will understand the 1's as positives and 0's as negatives. This is important when calculating metrics such as `precision` and `recall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metrics\n",
    "df_metrics[\"Purchase Canceled\"] = df_metrics[\"Purchase Canceled\"].astype(int)\n",
    "df_metrics[\"prediction\"] = df_metrics[\"prediction\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Profiling Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use a different method to profile performance data, `log_metrics`. We also need to define the feature that represents the labels, the predictions and the prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Dataset ID\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "print(\"Enter your Dataset ID\")\n",
    "datasetID = input()\n",
    "\n",
    "with session.logger(\n",
    "    # Note: 'datasetId' in whylogs maps to 'model-id' that is provided when you set up a model in WhyLabs\n",
    "    tags={\"datasetId\": datasetID}, dataset_timestamp=now\n",
    ") as ylog:\n",
    "    ylog.log_metrics(targets=df_metrics['Purchase Canceled'].tolist(), \n",
    "                predictions=df_metrics['prediction'].tolist(), \n",
    "                scores=df_metrics['score'].tolist(),\n",
    "                target_field=\"Purchase Canceled\",\n",
    "                prediction_field=\"prediction\",\n",
    "                score_field=\"Normalized Prediction Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the session once we're done.\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Inspecting your Model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showed the process for logging the performance metrics for a given day. We repeated the process for a number of consecutive days to show how we can inspect the calculated metrics on a daily basis in your model's dashboard at WhyLabs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/classification_metrics.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking only at the Accuracy metrics, it would look like our model has a good performance. However, by taking a closer look at the other metrics, we can see that our model is actually performing rather poorly. By looking at the confusion matrix, we can see that our data is very unbalanced, which is why the Accuracy gives such misleading results.\n",
    "\n",
    "The dashboard also presents us with other metrics as well. For classification tasks, the following metrics are tracked:\n",
    "\n",
    "- Total output and input count\n",
    "- Accuracy\n",
    "- ROC\n",
    "- Precision-Recall chart\n",
    "- Confusion Matrix\n",
    "- Recall\n",
    "- FPR (false positive rate)\n",
    "- Precision\n",
    "- F1\n",
    "\n",
    "\n",
    "You're free to inspect the rest of the metrics at your own dashboard. If you prefer, take a look at https://docs.whylabs.ai/docs/performance-metrics#classification for more information!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "323493c40bedb65fef2eec2a6e595ce0cca722dcb720da40c0d127c8422c938f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
