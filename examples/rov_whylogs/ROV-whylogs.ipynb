{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Kafka Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition, KafkaProducer\n",
    "import json\n",
    "\n",
    "def create_producer():\n",
    "    return KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "def create_consumer(topic):\n",
    "    consumer = KafkaConsumer(bootstrap_servers='localhost:9092', \n",
    "                         value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "    # Manually assign partitions\n",
    "    # https://github.com/dpkp/kafka-python/issues/601#issuecomment-331419097\n",
    "    assignments = []\n",
    "    partitions = consumer.partitions_for_topic(topic)\n",
    "    for p in partitions:\n",
    "        print(f'topic {topic} - partition {p}')\n",
    "        assignments.append(TopicPartition(topic, p))\n",
    "    consumer.assign(assignments)\n",
    "\n",
    "    return consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telemetry Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total>> 7912\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "producer = create_producer()\n",
    "\n",
    "foldername = 'rov_data'\n",
    "files = [str(i) for i in range(4)]\n",
    "\n",
    "sessions = []\n",
    "for ix,file in enumerate(files):\n",
    "    with open(os.path.join(foldername,'{}_injected.json'.format(file)),'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "        sessions.append([{\n",
    "                        'mtarg1':str(x['mtarg1']),\n",
    "                        'mtarg2':str(x['mtarg2']),\n",
    "                        'mtarg3':str(x['mtarg3']),\n",
    "\n",
    "                        'roll':x['roll'],\n",
    "                        'pitch':x['pitch'],\n",
    "                        'yaw':x['yaw'],\n",
    "\n",
    "                        'LACCX':x['LACCX'],\n",
    "                        'LACCY':x['LACCY'],\n",
    "                        'LACCZ':x['LACCZ'],\n",
    "\n",
    "                        'GYROX':x['GYROX'],\n",
    "                        'GYROY':x['GYROY'],\n",
    "                        'GYROZ':x['GYROZ'],\n",
    "\n",
    "                        'SC1I':x['SC1I'],\n",
    "                        'SC2I':x['SC2I'],\n",
    "                        'SC3I':x['SC3I'],\n",
    "\n",
    "                        'BT1I':x['BT1I'],\n",
    "                        'BT2I':x['BT2I'],\n",
    "\n",
    "                        'vout':x['vout'],\n",
    "                        'iout':x['iout'],\n",
    "                        'cpuUsage':x['cpuUsage'],\n",
    "            \n",
    "\n",
    "                        'timestamp':x['timestamp']} for x in raw_data['0']['data']])\n",
    "\n",
    "\n",
    "\n",
    "total = 0\n",
    "for session in sessions:\n",
    "    for sample in session:\n",
    "        producer.send('telemetry-rov', sample)\n",
    "        producer.flush()\n",
    "        total+=1\n",
    "print(\"Total>>\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"mtarg1\", \n",
    "    \"mtarg2\",  \n",
    "    \"mtarg3\",\n",
    "    \n",
    "    \"roll\",\n",
    "    \"pitch\",\n",
    "\n",
    "    \"LACCX\",          \n",
    "    \"LACCY\",  \n",
    "    \"LACCZ\",\n",
    "    \n",
    "    \"GYROX\",\n",
    "    \"GYROY\",  \n",
    "    \n",
    "    \"SC1I\",    \n",
    "    \"SC2I\",    \n",
    "    \"SC3I\",\n",
    "    \n",
    "    \n",
    "    \"BT1I\",    \n",
    "    \"BT2I\",    \n",
    "    \"vout\",    \n",
    "    \"iout\",    \n",
    "    \"cpuUsage\",\n",
    "        \n",
    "    ]\n",
    "\n",
    "fault_features = [\n",
    "    'fault',\n",
    "    'fault_type',\n",
    "    'fault_value',\n",
    "    'fault_duration'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic telemetry-rov - partition 0\n",
      "OVER TIME LIMIT\n",
      "OVER TIME LIMIT\n",
      "OVER TIME LIMIT\n",
      "OVER TIME LIMIT\n",
      "OVER TIME LIMIT\n",
      "OVER TIME LIMIT\n",
      "OVER TIME LIMIT\n",
      "Total>> 7912\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Loading regression model\n",
    "reg_model = joblib.load(open('model_files/BL_GYROZ.joblib', 'rb'))\n",
    "target = 'GYROZ'\n",
    "\n",
    "# Initializing Kafka consumer (for telemetry topic) and producker  (for prediction topic)\n",
    "topic = 'telemetry-rov'\n",
    "producer = create_producer()\n",
    "consumer = create_consumer(topic)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    consumer.seek_to_beginning()\n",
    "    session_number = 0\n",
    "    prev = None\n",
    "    total = 0\n",
    "    session_data = []\n",
    "    while True:\n",
    "        finished = True\n",
    "        record = consumer.poll(timeout_ms=500, max_records=100, update_offsets=True)\n",
    "        for k,v in record.items():\n",
    "            for row in v:\n",
    "                current = row.value\n",
    "                current_ts = row.value.get(\"timestamp\")\n",
    "                res = calculate_residual(current,prev)\n",
    "                to_send = {'residual':res,'timestamp':current_ts}\n",
    "                total +=1\n",
    "                producer.send('prediction-rov', to_send)\n",
    "                producer.flush()\n",
    "                prev = current\n",
    "                prev_ts = current_ts\n",
    "                finished = False\n",
    "        if finished:\n",
    "            break\n",
    "\n",
    "    print(\"Total>>\",total)\n",
    "\n",
    "def calculate_residual(current,prev):\n",
    "    if prev:\n",
    "        # More than 0.5s has passed without the ROV sending new telemetry data\n",
    "        if current['timestamp']-prev['timestamp']>500:\n",
    "            print(\"OVER TIME LIMIT\")\n",
    "            return np.nan\n",
    "        else:\n",
    "            prev = { ft: float(prev[ft]) for ft in features }\n",
    "            x = np.array(list(prev.values()))\n",
    "            x=x.reshape(1,-1)\n",
    "\n",
    "            # Min-max scaler for input features\n",
    "            with open('model_files/BL_x.pickle', 'rb') as f:\n",
    "                scaler_x=pickle.load(f)\n",
    "\n",
    "            x=scaler_x.transform(x)       \n",
    "\n",
    "            py = reg_model.predict(x)\n",
    "            # Min-max scaler for the target - GYROZ\n",
    "            with open('model_files/BL_y.pickle', 'rb') as f:\n",
    "                scaler_y=pickle.load(f)\n",
    "            py = py.reshape(1,-1)\n",
    "            py = scaler_y.inverse_transform(py)\n",
    "            py = py.ravel()\n",
    "            y = float(current[target])\n",
    "            # return residual (diff. between predicted and current)\n",
    "            return abs(py[0]-y)\n",
    "    else:\n",
    "        return np.nan\n",
    "if (__name__=='__main__'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from whylogs import get_or_create_session\n",
    "\n",
    "def break_into_batches(session_data):\n",
    "    min_sessions = []\n",
    "    min_session = []\n",
    "    for sample in session_data:\n",
    "        sample_ts = sample['timestamp']\n",
    "        try:\n",
    "            min_ts = min_session[0]['timestamp']\n",
    "        except:\n",
    "            min_ts = sample_ts\n",
    "        if sample_ts - min_ts < 60*1000:\n",
    "            min_session.append(sample)\n",
    "        else:\n",
    "            dt_object = datetime.fromtimestamp(min_ts/1000)\n",
    "            dt_str = datetime.strftime(dt_object, '%Y-%m-%d %H:%M')\n",
    "            dt_object = datetime.strptime(dt_str,'%Y-%m-%d %H:%M')\n",
    "            min_sessions.append((min_session,dt_object))\n",
    "            min_session = []\n",
    "            min_session.append(sample)\n",
    "    return min_sessions\n",
    "\n",
    "def log_session(dataset_name,session_data):\n",
    "    session = get_or_create_session()\n",
    "    session_batches = break_into_batches(session_data)\n",
    "    for min_batch in session_batches:\n",
    "        timestamp = min_batch[1]\n",
    "        with session.logger(dataset_name=dataset_name,dataset_timestamp=timestamp) as logger:\n",
    "            for r in min_batch[0]:\n",
    "                del r['timestamp']\n",
    "                logger.log(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telemetry Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic telemetry-rov - partition 0\n",
      "Logging session  0\n",
      "Logging session  1\n",
      "Logging session  2\n",
      "Logging session  3\n",
      "Total: 7912\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers='localhost:9092', \n",
    "                         value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "# consumer.seek_to_beginning workaround\n",
    "# https://github.com/dpkp/kafka-python/issues/601#issuecomment-331419097\n",
    "assignments = []\n",
    "topics=['telemetry-rov']\n",
    "for topic in topics:\n",
    "    partitions = consumer.partitions_for_topic(topic)\n",
    "    for p in partitions:\n",
    "        print(f'topic {topic} - partition {p}')\n",
    "        assignments.append(TopicPartition(topic, p))\n",
    "consumer.assign(assignments)\n",
    "\n",
    "consumer.seek_to_beginning()\n",
    "\n",
    "session_number = 0\n",
    "prev_ts = None\n",
    "total = 0\n",
    "session_data = []\n",
    "while True:\n",
    "    finished = True\n",
    "    record = consumer.poll(timeout_ms=500, max_records=100, update_offsets=True)\n",
    "    for k,v in record.items():\n",
    "        for row in v:\n",
    "            total+=1\n",
    "            current_ts = row.value.get('timestamp',None)\n",
    "            if not prev_ts:\n",
    "                prev_ts = current_ts\n",
    "\n",
    "            if abs(current_ts - prev_ts) > 5*60*1000:\n",
    "                print(\"Logging session \",session_number)\n",
    "                dataset_name = \"rov_telemetry_{}\".format(session_number)\n",
    "                log_session(dataset_name,session_data)\n",
    "\n",
    "                session_data = []\n",
    "                session_number+=1\n",
    "\n",
    "            session_data.append(row.value)\n",
    "            prev_ts = current_ts\n",
    "            finished = False\n",
    "\n",
    "    if finished:\n",
    "        print(\"Logging session \",session_number)\n",
    "        dataset_name = \"rov_telemetry_{}\".format(session_number)\n",
    "        log_session(dataset_name,session_data)\n",
    "\n",
    "        break\n",
    "\n",
    "print(\"Total:\",total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(slice_,wd):\n",
    "    if None in slice_:\n",
    "        return np.nan\n",
    "    if len(slice_) != wd:\n",
    "        return np.nan\n",
    "    return sum(slice_)/wd\n",
    "\n",
    "def add_moving_averages(resid_window,current):\n",
    "    r_5 = list(resid_window)[-5:]\n",
    "    r_10 = list(resid_window)[-10:]\n",
    "    r_15 = list(resid_window)[-15:]\n",
    "\n",
    "    current['residual_m5'] = moving_average(r_5,5)\n",
    "    current['residual_m10'] = moving_average(r_10,10)\n",
    "    current['residual_m15'] = moving_average(r_15,15)\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic prediction-rov - partition 0\n",
      "Logging session  0\n",
      "Logging session  1\n",
      "Logging session  2\n",
      "Logging session  3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import deque\n",
    "from logger_tools import log_session\n",
    "import numpy as np\n",
    "from kafkaConnector import create_consumer\n",
    "\n",
    "topic = 'prediction-rov'\n",
    "consumer = create_consumer(topic)\n",
    "\n",
    "resid_window = deque(maxlen=15)\n",
    "j = 0\n",
    "session_number = 0\n",
    "prev_ts = None\n",
    "session_data = []\n",
    "consumer.seek_to_beginning()\n",
    "while True:\n",
    "    finished = True\n",
    "    record = consumer.poll(timeout_ms=500, max_records=100, update_offsets=True)\n",
    "    for k,v in record.items():\n",
    "        for row in v:\n",
    "            current = row.value\n",
    "            current_ts = current['timestamp']\n",
    "            resid_window.append(current['residual'])\n",
    "            current = add_moving_averages(resid_window,current)\n",
    "\n",
    "            if not prev_ts:\n",
    "                prev_ts = current_ts\n",
    "\n",
    "            if abs(current_ts - prev_ts) > 5*60*1000:\n",
    "                print(\"Logging session \",session_number)\n",
    "                dataset_name = \"rov_prediction_{}\".format(session_number)\n",
    "                log_session(dataset_name,session_data)\n",
    "\n",
    "                session_data = []\n",
    "                session_number+=1\n",
    "            session_data.append(current)\n",
    "            prev_ts = current_ts\n",
    "            finished = False\n",
    "    if finished:\n",
    "        print(\"Logging session \",session_number)\n",
    "        dataset_name = \"rov_prediction_{}\".format(session_number)\n",
    "        log_session(dataset_name,session_data)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import some required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, fixed, IntSlider, HBox, Layout, Output, VBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display,Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to identify how many sessions we have, so the user can select the appropriate one.\n",
    "Let's sweep the content in the __whylogs_output__ folder and search for the relevant dataset names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "telemetry_sessions = [x for x in next(os.walk('whylogs-output'))[1] if 'rov_telemetry' in x]\n",
    "prediction_sessions = [x for x in next(os.walk('whylogs-output'))[1] if 'rov_prediction' in x]\n",
    "session_numbers = list(range(len(telemetry_sessions)))\n",
    "print(session_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to identify all of the available features in the __rov_telemetry__ sessions, because that will also be a user input.\n",
    "\n",
    "We'll do that by merging all of the __telemetry__ sessions, and getting the __columns__ proprerty, and then sorting it alphabetically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from whylogs import DatasetProfile\n",
    "from whylogs.viz import ProfileVisualizer\n",
    "\n",
    "features_per_session = []\n",
    "for s_no in session_numbers:\n",
    "    binaries = glob.glob('whylogs-output/rov_telemetry_{}/**/*.bin'.format(s_no), recursive=True)\n",
    "    binaries\n",
    "    # currently, whylogs writer writes non-delimited files\n",
    "    profiles = [DatasetProfile.read_protobuf(x, delimited_file=False) for x in binaries]\n",
    "    from functools import reduce\n",
    "    merged = reduce(lambda x, y: x.merge(y), profiles)\n",
    "    merged.columns\n",
    "    # viz = ProfileVisualizer()\n",
    "    # viz.set_profiles(profiles)\n",
    "    # viz.plot_distribution(\"GYROZ\")\n",
    "    feature_list = sorted([x for x in merged.columns])\n",
    "    features_per_session.append(feature_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another user input is the sliding window of the moving average. This will depend on the __predict_logger__, so we'll need to inspect the available features of the __rov_prediction__ topic, which will have features of the form:\n",
    "\n",
    "residual_m{}\n",
    "\n",
    "Where {} is an int, indication the window size for the moving average.\n",
    "\n",
    "To do that, we'll merge all of the __prediction__ sessions, get the list of columns, and then split by the \"__\\___\" character, to get the last element and find the window sizes we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from whylogs import DatasetProfile\n",
    "from whylogs.viz import ProfileVisualizer\n",
    "\n",
    "def to_numeric(avg_windows_per_session):\n",
    "    new_avg_windows_per_session = []\n",
    "    for avg_list in avg_windows_per_session:\n",
    "        new_avg_list = []\n",
    "        for res in avg_list:\n",
    "            try:\n",
    "                wd = int(res.split(\"_\")[-1][1:])\n",
    "            except:\n",
    "                wd = 1\n",
    "            new_avg_list.append(wd)\n",
    "        new_avg_windows_per_session.append(sorted(new_avg_list))\n",
    "    return new_avg_windows_per_session\n",
    "\n",
    "avg_windows_per_session = []\n",
    "for s_no in session_numbers:\n",
    "    binaries = glob.glob('whylogs-output/rov_prediction_{}/**/*.bin'.format(s_no), recursive=True)\n",
    "    binaries\n",
    "    # currently, whylogs writer writes non-delimited files\n",
    "    profiles = [DatasetProfile.read_protobuf(x, delimited_file=False) for x in binaries]\n",
    "    from functools import reduce\n",
    "    merged = reduce(lambda x, y: x.merge(y), profiles)\n",
    "    merged.columns\n",
    "    # viz = ProfileVisualizer()\n",
    "    # viz.set_profiles(profiles)\n",
    "    # viz.plot_distribution(\"GYROZ\")\n",
    "    avg_list = [x for x in merged.columns]\n",
    "    avg_windows_per_session.append(avg_list)\n",
    "\n",
    "avg_windows_per_session = to_numeric(avg_windows_per_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have all the information to actually build the widget.\n",
    "\n",
    "Let's define our user inputs and how we will make them available for the user:\n",
    "- Session Number : Dropdown\n",
    "- Feature List for Telemetry Topic: Dropdown\n",
    "- Plot Type for Telemetry Features: Dropdown\n",
    "- Window Size for the Residual's Moving Average: Slider\n",
    "- Plot Type for Prediction Features: Dropdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import glob\n",
    "from whylogs import DatasetProfile\n",
    "from whylogs.viz import ProfileVisualizer\n",
    "from datetime import datetime\n",
    "\n",
    "plot_types = [\"Distribution\",\"Data Types\",\"Missing Values\",\"Uniqueness\"]\n",
    "plot_types_res = [\"Distribution\",\"Missing Values\"]\n",
    "\n",
    "dropdown_plots = widgets.Dropdown(options = plot_types,value=\"Distribution\",description='Plot Type')\n",
    "dropdown_plots_res = widgets.Dropdown(options = plot_types_res,value=\"Distribution\",description='Plot Type')\n",
    "\n",
    "dropdown_session = widgets.Dropdown(options = session_numbers,description='Session No.')\n",
    "dropdown_features = widgets.Dropdown(options = features_per_session[dropdown_session.value],value=\"GYROZ\",description='Feature:')\n",
    "slider_avg=widgets.SelectionSlider(description='Mov. Avg.',   options=avg_windows_per_session[dropdown_session.value])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have basically, two main areas of Output:\n",
    "- output_telemetry\n",
    "- output_prediction\n",
    "\n",
    "In `output_telemetry`, we'll display:\n",
    "- Session Date\n",
    "- The chosen Plot for the chosen Feature\n",
    "\n",
    "In `output_prediction`, we'll display:\n",
    "- The chosen Plot for the residual, applying the chosen moving average\n",
    "\n",
    "We separate that creating a `widgets.Output`, and calling `with output_xxx` in order to display things in the right order: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_telemetry = widgets.Output(layout={'width': 'auto','align_items':'center','object_fit':'fill','margin':'0px 10px 10px 0px'})\n",
    "output_prediction = widgets.Output(layout={'width': 'auto','align_items':'center','object_fit':'fill','margin':'0px 10px 10px 0px'})\n",
    "\n",
    "def plot_feature_distribution(session_number,feature_name,plot_type):\n",
    "#     print(\"change new>\",change.new)\n",
    "\n",
    "    with output_telemetry:\n",
    "        output_telemetry.clear_output(wait=True)\n",
    "\n",
    "#         display(change.new)\n",
    "#         output_session.clear_output()\n",
    "\n",
    "#         print(\"change new>\",change.new)\n",
    "        telemetry_session = \"rov_telemetry_{}\".format(session_number)\n",
    "        binaries = glob.glob('whylogs-output/{}/**/*.bin'.format(telemetry_session), recursive=True)\n",
    "        binaries\n",
    "        # currently, whylogs writer writes non-delimited files\n",
    "        profiles = [DatasetProfile.read_protobuf(x, delimited_file=False) for x in binaries]\n",
    "        profiles\n",
    "        display(Markdown(\"# Session Date:\"))\n",
    "        dt_object = profiles[0].dataset_timestamp \n",
    "        dt_str = datetime.strftime(dt_object, '%Y-%m-%d %H:%M')\n",
    "        display(Markdown(\"%s\"%(dt_str)))\n",
    "        display(Markdown(\"# Telemetry\"))\n",
    "\n",
    "        viz = ProfileVisualizer()\n",
    "        viz.set_profiles(profiles)\n",
    "        if plot_type == \"Distribution\":\n",
    "            display(viz.plot_distribution(feature_name,ts_format='%H:%M'))\n",
    "        if plot_type == \"Data Types\":\n",
    "            display(viz.plot_data_types(feature_name,ts_format='%H:%M'))\n",
    "        if plot_type == \"Missing Values\":\n",
    "            display(viz.plot_missing_values(feature_name,ts_format='%H:%M'))\n",
    "        if plot_type == \"Uniqueness\":\n",
    "            display(viz.plot_uniqueness(feature_name,ts_format='%H:%M'))\n",
    "            \n",
    "\n",
    "def plot_residual_distribution(session_number,wd,plot_type):\n",
    "#     print(\"change new>\",change.new)\n",
    "\n",
    "    with output_prediction:\n",
    "        output_prediction.clear_output(wait=True)\n",
    "        display(Markdown(\"# Residuals\"))\n",
    "        telemetry_session = \"rov_prediction_{}\".format(session_number)\n",
    "        binaries = glob.glob('whylogs-output/{}/**/*.bin'.format(telemetry_session), recursive=True)\n",
    "        binaries\n",
    "        # currently, whylogs writer writes non-delimited files\n",
    "        profiles = [DatasetProfile.read_protobuf(x, delimited_file=False) for x in binaries]\n",
    "        profiles\n",
    "        viz = ProfileVisualizer()\n",
    "        viz.set_profiles(profiles)\n",
    "        if wd == 1:\n",
    "            resid_name = \"residual\"\n",
    "        else:\n",
    "            resid_name = \"residual_m{}\".format(wd)\n",
    "        if plot_type == \"Distribution\":\n",
    "            display(viz.plot_distribution(resid_name,ts_format='%H:%M'))\n",
    "        if plot_type == \"Missing Values\":\n",
    "            display(viz.plot_missing_values(resid_name,ts_format='%H:%M'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to listen for changes in the input widgets (dropdowns and sliders), and call the `plot_feature_distribution` and `plot_residual_distribution` whenever there is a change, which will display our plots in the appropriate outputs.\n",
    "\n",
    "We do that by defining an `eventhandler` for each input widget. These handlers will be called whenever there's a change in the respective input widget. In each `eventhandler`, we'll make it plot both `telemetry` and `prediction` sections, with the updated values (which will always be the changed value of the respective input widget, and the current state for the rest of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropdown_session_eventhandler(change):\n",
    "    plot_feature_distribution(change.new,dropdown_features.value,dropdown_plots.value)\n",
    "    plot_residual_distribution(change.new,slider_avg.value,dropdown_plots_res.value)\n",
    "\n",
    "def dropdown_plots_eventhandler(change):\n",
    "    plot_feature_distribution(dropdown_session.value,dropdown_features.value,change.new)\n",
    "    plot_residual_distribution(dropdown_session.value,slider_avg.value,dropdown_plots_res.value)\n",
    "\n",
    "def dropdown_plots_res_eventhandler(change):\n",
    "    plot_residual_distribution(dropdown_session.value,slider_avg.value,change.new)\n",
    "    plot_feature_distribution(dropdown_session.value,dropdown_features.value,dropdown_plots.value)\n",
    "\n",
    "def dropdown_features_eventhandler(change):\n",
    "    plot_feature_distribution(dropdown_session.value,change.new,dropdown_plots.value)\n",
    "    plot_residual_distribution(dropdown_session.value,slider_avg.value,dropdown_plots_res.value)\n",
    "\n",
    "def slider_window_eventhandler(change):\n",
    "    plot_residual_distribution(dropdown_session.value,change.new,dropdown_plots_res.value)\n",
    "    plot_feature_distribution(dropdown_session.value,dropdown_features.value,dropdown_plots.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the `eventhandlers`, we need to set the inputs to observe them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_session.observe(dropdown_session_eventhandler, names='value')\n",
    "dropdown_features.observe(dropdown_features_eventhandler, names='value')\n",
    "dropdown_plots.observe(dropdown_plots_eventhandler, names='value')\n",
    "dropdown_plots_res.observe(dropdown_plots_res_eventhandler, names='value')\n",
    "slider_avg.observe(slider_window_eventhandler,names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to centralize all inputs and outputs in a single Box.\n",
    "\n",
    "We do that by creating a VBox, setting some layouts configurations for it, and then informing what will be on the VBox.\n",
    "\n",
    "V is for Vertical, so the arguments we send to it will be displayed vertically and in order.\n",
    "\n",
    "We want to show the following elements in this order:\n",
    "- Session Dropdown (INPUT)\n",
    "- Telemetry Plot (OUTPUT)\n",
    "- Feature List (INPUT)\n",
    "- Plot type for Telemetry (INPUT)\n",
    "- Prediction Plot (OUTPUT)\n",
    "- Plot type for Prediction (INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077271d21efc4a40a127a6d81e941302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Session No.', index=1, options=(0, 1, 2, 3), value=1), Output(layout=Layoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, fixed, IntSlider, HBox, Layout, Output, VBox\n",
    "import ipywidgets as widgets\n",
    "box_layout = Layout(\n",
    "                    align_items='center',\n",
    "                    justify_content='space-around',\n",
    "                    display='flex',\n",
    "                    border='solid',\n",
    "                    )\n",
    "box = VBox([dropdown_session,output_telemetry,dropdown_features,dropdown_plots,output_prediction,dropdown_plots_res,slider_avg],layout=box_layout)\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
