{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow + WhyLabs Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial showcases how you can use the whylabs integration to:\n",
    "* Capture data quality metrics while training a linear regression model in `mlflow`\n",
    "* Extract whylogs data back into an in-memory format from the MLflow backend\n",
    "* Log same data back into whylabs platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this tutorial:\n",
    "* Create and then activate the conda environment included using `environment.yml` file and run this notebook using this new environment `whylabs-mlflow` as kernel.\n",
    "* You'll need to install pip into the conda environment using `conda install pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "First, we want to filter out noisy warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import whylogs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from whylogs import get_or_create_session\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env_mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Enable whylogs Integration\n",
    "\n",
    "Enable whylogs in MLflow to allow storing whylogs statistical profiles with every run. This method returns `True` if whylogs is able to patch MLflow. You might want to pass a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_or_create_session(\".whylogs_mlflow.yaml\")\n",
    "whylogs.enable_mlflow(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "Download and prepare the UCI wine quality dataset. We sample the test dataset further to represent batches of data produced every second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.environ[\"DATASET_URL\"], sep=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train, test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relocate predicted variable \"quality\" to y vectors\n",
    "train_x = train.drop([\"quality\"], axis=1).reset_index(drop=True)\n",
    "test_x = test.drop([\"quality\"], axis=1).reset_index(drop=True)\n",
    "train_y = train[[\"quality\"]].reset_index(drop=True)\n",
    "test_y = test[[\"quality\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the data to simulate multiple machine learning model learning looking for the best hyperparameters (simulation of an hyperparameter optimization processs).\n",
    "\n",
    "First, we split our test sets data to test every algorithm (also, for testing whylogs ability to log dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLflow experiment for our demo\n",
    "experiment_name = \"whylogs demo\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "model_params = {\"alpha\": 1.0,\n",
    "                \"l1_ratio\": 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further evaluate the model performance using crossvalidation with 10 folds without overlapping, taking each batch, evaluating the model and then averaging the metrics obtained for each batch.\n",
    "\n",
    "It's essential to index the data given a timestamp index because information is indexed by timestamp in WhyLabs Platform.\n",
    "\n",
    "Note that whylogs profiler data is automatically logged when mlflow.end_run() is called implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds)\n",
    "mae_v = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_x)):\n",
    "    with mlflow.start_run(run_name=f\"Run {i + 1}\"):        \n",
    "        X_train, X_test = train_x.loc[train_index], train_x.loc[test_index]\n",
    "        y_train, y_test = train_y.loc[train_index], train_y.loc[test_index]\n",
    "        \n",
    "        # Train a model with each split\n",
    "        lr = ElasticNet(**model_params)\n",
    "        lr.fit(X_train, y_train)\n",
    "        print(\"ElasticNet model (%s):\" % model_params)\n",
    "        \n",
    "        # Evaluate trained model \n",
    "        predicted_output = lr.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, predicted_output)\n",
    "\n",
    "        # Log to mlflow the hyperparameters and evaluation metric\n",
    "        mlflow.log_params(model_params)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        X_test[0, \"mae\"] = mae\n",
    "        \n",
    "        for k, v in model_params.items():\n",
    "            train_x.iloc[test_index][k] = v\n",
    "        \n",
    "        # use whylogs to log data quality metrics for the current batch\n",
    "        mlflow.whylogs.log_pandas(\n",
    "            train_x.iloc[test_index], \n",
    "            datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=i)\n",
    "        )\n",
    "        print(\"Subset %.0f, mean absolute error: %s\" % (i + 1, mae))\n",
    "        mae_v.append(mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
