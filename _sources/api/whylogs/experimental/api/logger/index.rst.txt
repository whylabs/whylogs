:py:mod:`whylogs.experimental.api.logger`
=========================================

.. py:module:: whylogs.experimental.api.logger


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   whylogs.experimental.api.logger.SegmentedResultSet
   whylogs.experimental.api.logger.ViewResultSet
   whylogs.experimental.api.logger.DatasetSchema
   whylogs.experimental.api.logger.RowWiseMetrics



Functions
~~~~~~~~~

.. autoapisummary::

   whylogs.experimental.api.logger.log
   whylogs.experimental.api.logger.log_batch_ranking_metrics



Attributes
~~~~~~~~~~

.. autoapisummary::

   whylogs.experimental.api.logger.diagnostic_logger


.. py:function:: log(obj: Any = None, *, pandas: Optional[whylogs.core.stubs.pd.DataFrame] = None, row: Optional[Dict[str, Any]] = None, schema: Optional[whylogs.core.DatasetSchema] = None, name: Optional[str] = None, multiple: Optional[Dict[str, Loggable]] = None, dataset_timestamp: Optional[datetime.datetime] = None, trace_id: Optional[str] = None, tags: Optional[List[str]] = None, segment_key_values: Optional[Dict[str, str]] = None, debug_event: Optional[Dict[str, Any]] = None) -> result_set.ResultSet


.. py:class:: SegmentedResultSet(segments: Dict[str, Dict[whylogs.core.Segment, Union[whylogs.core.DatasetProfile, whylogs.core.DatasetProfileView]]], partitions: Optional[List[whylogs.core.segmentation_partition.SegmentationPartition]] = None, metrics: Optional[Dict[str, Any]] = None, properties: Optional[Dict[str, Any]] = None)

   Bases: :py:obj:`ResultSet`

   A holder object for profiling results.

   A whylogs.log call can result in more than one profile. This wrapper class
   simplifies the navigation among these profiles.

   Note that currently we only hold one profile but we're planning to add other
   kinds of profiles such as segmented profiles here.

   .. py:property:: dataset_properties
      :type: Optional[Dict[str, Any]]


   .. py:property:: dataset_metrics
      :type: Optional[Dict[str, Any]]


   .. py:property:: partitions
      :type: Optional[List[whylogs.core.segmentation_partition.SegmentationPartition]]


   .. py:property:: count
      :type: int


   .. py:property:: model_performance_metric
      :type: Optional[whylogs.core.model_performance_metrics.ModelPerformanceMetrics]


   .. py:property:: metadata
      :type: Optional[Dict[str, str]]


   .. py:property:: performance_metrics
      :type: Optional[whylogs.core.model_performance_metrics.ModelPerformanceMetrics]


   .. py:method:: profile(segment: Optional[whylogs.core.Segment] = None) -> Optional[Union[whylogs.core.DatasetProfile, whylogs.core.DatasetProfileView]]


   .. py:method:: get_writables() -> Optional[List[whylogs.api.writer.writer._Writable]]


   .. py:method:: get_whylabs_tags() -> List[whylabs_client.model.segment_tag.SegmentTag]


   .. py:method:: get_timestamps() -> List[Optional[datetime.datetime]]


   .. py:method:: set_dataset_timestamp(dataset_timestamp: datetime.datetime) -> None


   .. py:method:: segments(restrict_to_parition_id: Optional[str] = None) -> Optional[List[whylogs.core.Segment]]


   .. py:method:: segments_in_partition(partition: whylogs.core.segmentation_partition.SegmentationPartition) -> Optional[Dict[whylogs.core.Segment, Union[whylogs.core.DatasetProfile, whylogs.core.DatasetProfileView]]]


   .. py:method:: view(segment: Optional[whylogs.core.Segment] = None) -> Optional[whylogs.core.DatasetProfileView]


   .. py:method:: get_model_performance_metrics_for_segment(segment: whylogs.core.Segment) -> Optional[whylogs.core.model_performance_metrics.ModelPerformanceMetrics]


   .. py:method:: add_metrics_for_segment(metrics: whylogs.core.model_performance_metrics.ModelPerformanceMetrics, segment: whylogs.core.Segment) -> None


   .. py:method:: zero() -> SegmentedResultSet
      :staticmethod:


   .. py:method:: add_model_performance_metrics(metrics: whylogs.core.model_performance_metrics.ModelPerformanceMetrics) -> None


   .. py:method:: add_metric(name: str, metric: whylogs.core.metrics.metrics.Metric) -> None


   .. py:method:: merge(other: ResultSet) -> SegmentedResultSet


   .. py:method:: get_default_path() -> Optional[str]


   .. py:method:: read(multi_profile_file: str) -> ResultSet
      :staticmethod:


   .. py:method:: reader(name: str = 'local') -> ResultSetReader
      :staticmethod:


   .. py:method:: write(path: Optional[str] = None, **kwargs: Any) -> Tuple[bool, str]


   .. py:method:: writer(name: str = 'local', **kwargs: Any) -> WriterWrapper

      Utility method to create a Writer of the specified type



.. py:class:: ViewResultSet(view: whylogs.core.DatasetProfileView)

   Bases: :py:obj:`ResultSet`

   A holder object for profiling results.

   A whylogs.log call can result in more than one profile. This wrapper class
   simplifies the navigation among these profiles.

   Note that currently we only hold one profile but we're planning to add other
   kinds of profiles such as segmented profiles here.

   .. py:property:: metadata
      :type: Optional[Dict[str, str]]


   .. py:property:: count
      :type: int


   .. py:property:: performance_metrics
      :type: Optional[whylogs.core.model_performance_metrics.ModelPerformanceMetrics]


   .. py:method:: profile() -> Optional[whylogs.core.DatasetProfile]


   .. py:method:: view() -> Optional[whylogs.core.DatasetProfileView]


   .. py:method:: zero() -> ViewResultSet
      :staticmethod:


   .. py:method:: merge(other: ResultSet) -> ViewResultSet


   .. py:method:: set_dataset_timestamp(dataset_timestamp: datetime.datetime) -> None


   .. py:method:: get_default_path() -> Optional[str]


   .. py:method:: read(multi_profile_file: str) -> ResultSet
      :staticmethod:


   .. py:method:: reader(name: str = 'local') -> ResultSetReader
      :staticmethod:


   .. py:method:: get_writables() -> Optional[List[whylogs.api.writer.writer._Writable]]


   .. py:method:: add_model_performance_metrics(metrics: whylogs.core.model_performance_metrics.ModelPerformanceMetrics) -> None


   .. py:method:: add_metric(name: str, metric: whylogs.core.metrics.metrics.Metric) -> None


   .. py:method:: write(path: Optional[str] = None, **kwargs: Any) -> Tuple[bool, str]


   .. py:method:: writer(name: str = 'local', **kwargs: Any) -> WriterWrapper

      Utility method to create a Writer of the specified type



.. py:class:: DatasetSchema(types: Optional[Dict[str, Any]] = None, default_configs: Optional[whylogs.core.metrics.metrics.MetricConfig] = None, type_mapper: Optional[whylogs.core.datatypes.TypeMapper] = None, resolvers: Optional[whylogs.core.resolvers.Resolver] = None, cache_size: int = 1024, schema_based_automerge: bool = False, segments: Optional[Dict[str, whylogs.core.segmentation_partition.SegmentationPartition]] = None, validators: Optional[Dict[str, List[whylogs.core.validators.validator.Validator]]] = None, metadata: Optional[Dict[str, str]] = None)

   Defines the schema for tracking metrics in whylogs.

   In order to customize your tracking, you can extend this class to specify your own column schema
   or your own type resolution. Otherwise, you can just use the default DatasetSchema object.

   Schema objects are also used to group datasets together.

   .. attribute:: types

      required. a dictionay of column name to the Python type.

   .. attribute:: default_configs

      optional. Options to configure various behavior of whylogs.

   .. attribute:: type_mapper

      Optional. a mapper that transates the Python type to standardized whylogs :class:`DataType` object.

   .. attribute:: resolvers

      Optional. an object that defines how to map from a column name, a whylogs :class:`DataType` and a
      schema to metrics.

   .. rubric:: Examples

   >>> import pandas as pd
   >>> import numpy as np
   >>> from whylogs.core import DatasetSchema, DatasetProfile
   >>> from whylogs.core.resolvers import Resolver, StandardResolver
   >>>
   >>> class MyResolver(StandardResolver):
   ...    pass
   >>>
   >>> schema = DatasetSchema(
   ...    types={
   ...        "col1": str,
   ...        "col2": np.int32,
   ...        "col3": pd.CategoricalDtype(categories=('foo', 'bar'), ordered=True)
   ...    },
   ...    resolvers=MyResolver()
   ... )
   >>> prof = DatasetProfile(schema)
   >>> df = pd.DataFrame({"col1": ['foo'], "col2": np.array([1], dtype=np.int32), "col3": ['bar']})
   >>> prof.track(pandas=df)

   .. py:method:: copy() -> DatasetSchema

      Returns a new instance of the same underlying schema


   .. py:method:: resolve(*, pandas: Optional[whylogs.core.stubs.pd.DataFrame] = None, row: Optional[Mapping[str, Any]] = None) -> bool


   .. py:method:: get_col_names() -> tuple


   .. py:method:: get(name: str) -> Optional[ColumnSchema]



.. py:data:: diagnostic_logger

   

.. py:class:: RowWiseMetrics(target_column: str, prediction_column: str, convert_non_numeric: bool = False)

   .. py:method:: relevant_counter(row: whylogs.core.stubs.pd.core.series.Series, k: int) -> int


   .. py:method:: sum_gains(row: whylogs.core.stubs.pd.core.series.Series, k: int) -> int


   .. py:method:: is_k_item_relevant(row: whylogs.core.stubs.pd.core.series.Series, k: int) -> int


   .. py:method:: get_top_rank(row: whylogs.core.stubs.pd.core.series.Series, k: int) -> Optional[int]


   .. py:method:: calc_non_numeric_relevance(row_dict: whylogs.core.stubs.pd.core.series.Series) -> Tuple[List[int], List[int]]


   .. py:method:: calculate_row_ndcg(row_dict: whylogs.core.stubs.pd.core.series.Series, k: int) -> float



.. py:function:: log_batch_ranking_metrics(data: whylogs.core.stubs.pd.core.frame.DataFrame, prediction_column: Optional[str] = None, target_column: Optional[str] = None, score_column: Optional[str] = None, k: Optional[int] = None, schema: Union[whylogs.core.DatasetSchema, None] = None, log_full_data: bool = False) -> whylogs.api.logger.result_set.ViewResultSet

   Log ranking metrics for a batch of data.

   You can call the function several ways:
     - Pass both prediction_column and target_column.
         - The named columns contain lists of strings. In this case, the prediction column contains the
           items the model has predicted are relevant, and the target column contains the items that
           are actually relevant. In this case, relevance is boolean.

         - The prediction column contains lists of integers and the target column contains lists of numbers
           or booleans. The value at the i-th position in the predicted list is the predicted rank of the i-th
           element of the domain. The value at the i-th position in the target list is the true relevance score of the
           i-th element of the domain. The score can be numeric or boolean. Higher scores indicate higher relevance.

     - Pass both target_column and score_column. The value at the i-th position in the target list is the true relevance
       of the i-th element of the domain (represented as a number, higher being more relevant; or boolean). The value at
       the i-th position in the score list is the model output for the i-th element of the domain.

     - Pass only target_column. The target column contians lists of numbers or booleans. The list entries are the true
       relevance of the items predicted by the model in prediction order.

   :param data: Dataframe with the data to log.
   :type data: pd.core.frame.DataFrame
   :param prediction_column: Column name for the predicted values. If not provided, the score_column and target_column must be provided, by default None
   :type prediction_column: Optional[str], optional
   :param target_column: Column name for the relevance scores. If not provided, relevance must be encoded within prediction column, by default None
   :type target_column: Optional[str], optional
   :param score_column: Column name for the scores. Can either be probabilities, confidence values, or other continuous measures.
                        If not passed, prediction_column must be passed,by default None
   :type score_column: Optional[str], optional
   :param k: Consider the top k ranks for metrics calculation.
             If `None`, use all outputs, by default None
   :type k: Optional[int], optional
   :param schema: Defines the schema for tracking metrics in whylogs, by default None
   :type schema: Union[DatasetSchema, None], optional
   :param log_full_data: Whether to log the complete dataframe or not.
                         If True, the complete DF will be logged in addition to the ranking metrics.
                         If False, only the calculated ranking metrics will be logged.
                         In a typical production use case, the ground truth might not be available
                         at the time the remaining data is generated. In order to prevent double profiling the
                         input features, consider leaving this as False. by default False
   :type log_full_data: bool, optional

   :returns:
   :rtype: ViewResultSet

   .. rubric:: Examples

   ::

       import pandas as pd
       from whylogs.experimental.api.logger import log_batch_ranking_metrics

       # 1st and 2nd recommended items are relevant - 3rd is not
       df = pd.DataFrame({"targets": [[1, 0, 1]], "predictions": [[2,3,1]]})
       results = log_batch_ranking_metrics(
           data=df,
           prediction_column="predictions",
           target_column="targets",
           k=3,
       )

   ::

       non_numerical_df = pd.DataFrame(
           {
               "raw_predictions": [
                   ["cat", "pig", "elephant"],
                   ["horse", "donkey", "robin"],
               ],
               "raw_targets": [
                   ["cat", "elephant"],
                   ["dog"],
               ],
           }
       )

       # 1st query:
       # Recommended items: [cat, pig, elephant]
       # Relevant items: [cat, elephant]

       # 2nd query:
       # Recommended items: [horse, donkey, robin]
       # Relevant items: [dog]

       results = log_batch_ranking_metrics(
           k=2,
           data=non_numerical_df,
           prediction_column="raw_predictions",
           target_column="raw_targets",
           convert_non_numeric=True
       )

   ::

       binary_single_df = pd.DataFrame(
           {
               "raw_targets": [
                   [True, False, True], # First recommended item: Relevant, Second: Not relevant, Third: Relevant
                   [False, False, False], # None of the recommended items are relevant
                   [True, True, False], # First and second recommended items are relevant
               ]
           }
       )

       result = log_batch_ranking_metrics(data=binary_single_df, target_column="raw_targets", k=3)



