{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
    ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Writing_Ranking_Performance_Metrics_to_WhyLabs)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Writing_Ranking_Performance_Metrics_to_WhyLabs) to leverage the power of whylogs and WhyLabs together!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Ranking Model Performance Metrics\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/experimental/Writing_Rank_Performance_Metrics_to_WhyLabs.ipynb)\n",
    "\n",
    "In this tutorial, we'll show how you can log ranking metrics of your ML Model with whylogs, and how to send it to your dashboard at Whylabs Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing whylogs\n",
    "\n",
    "First, let's install whylogs. Since we want to write to WhyLabs, we'll install the whylabs extra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  whylogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import whylogs as why"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "\n",
    "We see three data formats as example data. Notice that the predictions AND targets columns can take lists or np.ndarrays of values.\n",
    "\n",
    "Consider the predictions for each row to be a list of ranked results in decreasing rank importance order (best match is first). In the case of the `binary` example, we pass predictions with a boolean representing whether or not the prediction was successful, e.g., relevant search results.\n",
    "\n",
    "Consider targets lists to be essentially sets of values that denote successful predictions. These can be values (matched based on equality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single predictions\n",
    "single_df = pd.DataFrame({\"raw_predictions\": [[\"cat\", \"pig\", \"elephant\"], [\"horse\", \"donkey\", \"robin\"],\n",
    "                                          [\"cow\", \"pig\", \"giraffe\"], [\"pig\", \"dolphin\", \"elephant\"]],\n",
    "                          \"raw_targets\": [\"cat\", \"dog\", \"pig\", \"elephant\"]})\n",
    "\n",
    "# Binary predictions (True representing relevant results, False representing irrelevant)\n",
    "binary_df = pd.DataFrame({\"raw_predictions\": [[True, False, True], [False, False, False],\n",
    "                                          [True, True, False], [False, True, False]]})\n",
    "\n",
    "# Multiple predictions\n",
    "multiple_df = pd.DataFrame({\"raw_targets\": [[\"cat\", \"elephant\"], [\"dog\", \"pig\"],\n",
    "                                            [\"pig\", \"cow\"], [\"cat\", \"dolphin\"]],\n",
    "                            \"raw_predictions\": [[\"cat\", \"pig\", \"elephant\"], [\"horse\", \"donkey\", \"robin\"],\n",
    "                                                [\"cow\", \"pig\", \"giraffe\"], [\"pig\", \"dolphin\", \"elephant\"]]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log using experimental batch ranking metrics API\n",
    "\n",
    "It's noteworthy that this API is for batch metrics -- meaning that the metrics are not meant to be merged together. This means that multiple profiles from distributed machines, Spark, or multiple uploads within the model granularity window are discouraged.\n",
    "\n",
    "Contrastingly, non-batch metrics APIs can be merged (but one is not yet released for ranking metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log ranking metrics\n",
    "from whylogs.experimental.api.logger import log_batch_ranking_metrics\n",
    "\n",
    "results = log_batch_ranking_metrics(\n",
    "    k=2,\n",
    "    data=single_df,\n",
    "    prediction_column=\"raw_predictions\",\n",
    "    target_column=\"raw_targets\",\n",
    "    log_full_data=True\n",
    ")\n",
    "\n",
    "# NOTE: If you've already ran why.log() on your input data, change log_full_data to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "results.view().to_pandas().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send profile summaries to WhyLabs platform\n",
    "Required information can be found at https://hub.whylabsapp.com under the Settings > Model and Dataset Management page.\n",
    "\n",
    "API keys are only shown once, so you may need to create a new one and save somewhere safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure WhyLabs info, if needed\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input(\"Enter your WhyLabs Org ID\")\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = input(\"Enter your WhyLabs Dataset ID\")\n",
    "os.environ[\"WHYLABS_API_KEY\"] =  getpass.getpass(\"Enter your WhyLabs API key\")\n",
    "print(\"Using API Key ID: \", os.environ[\"WHYLABS_API_KEY\"][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results to WhyLabs platform\n",
    "results.writer(\"whylabs\").write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking metrics should be available in the WhyLabs platform, under the __Performance__ tab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wl-public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
