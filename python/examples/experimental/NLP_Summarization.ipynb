{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_hazICzT0AX"
      },
      "source": [
        ">### ðŸš© *Create a free WhyLabs account to get more value out of whylogs!*<br> \n",
        ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Schema_Configuration)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=whylogs-Github&utm_medium=whylogs-example&utm_campaign=Schema_Configuration) to leverage the power of whylogs and WhyLabs together!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phJi2VWRUEio"
      },
      "source": [
        "# Document Summarization Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_TgL10MUrSZ"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/whylogs/blob/mainline/python/examples/basic/Schema_Configuration.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH4hheWfUt7-"
      },
      "source": [
        "In this example, we'll look at how we might use whylogs to monitor a document summarization task.\n",
        "\n",
        "We'll use [NLTK](https://www.nltk.org) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to do some of the basic NLP tasks, so let's install the packages we'll need now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw9ErM4ZTpZo",
        "outputId": "46b881df-76f6-452f-b2d0-3ade1e4e3b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/bernease/miniconda3/envs/wl-public/lib/python3.8/site-packages (3.7)\n",
            "Requirement already satisfied: click in /home/bernease/miniconda3/envs/wl-public/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /home/bernease/miniconda3/envs/wl-public/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /home/bernease/miniconda3/envs/wl-public/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/bernease/miniconda3/envs/wl-public/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132 kB 3.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Using cached soupsieve-2.4-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1255 sha256=d638bb5299faa0db71a94164142bcf23b1438e67d1570ac1a115357111df33e4\n",
            "  Stored in directory: /home/bernease/.cache/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
            "Successfully built bs4\n",
            "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
            "Successfully installed beautifulsoup4-4.12.0 bs4-0.0.1 soupsieve-2.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install bs4\n",
        "#%pip install whylogs[embeddings]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1D0VHR4YsAC"
      },
      "source": [
        "We'll use the NLTK Reuters corpus as the documents to summarize. As a trivial summarization algorithm, we'll pull out the sentence that contains a document's highest log-entropy weighted term as its summary. Let's start by computing the term-frequency index for the corpus and the term global frequencies and entropies. We'll use NLTK's stemming, stopping, and tokenization for those calcuations, but return the unaltered sentence as the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikXbxGhGaq3d",
        "outputId": "1393b277-ead1-4ae1-88a0-5b8f30bd6924"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /home/bernease/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/bernease/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/bernease/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/tmp/ipykernel_1624/3218619406.py:63: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  raw = BeautifulSoup(delete_headline(reuters.raw(file)), \"html.parser\").get_text()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500 articles   6275 vocabulary\n"
          ]
        }
      ],
      "source": [
        "from typing import Any, Dict, List, Optional, Set\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "STEMMER = nltk.stem.PorterStemmer()\n",
        "\n",
        "# the NLTK tokenizer produces some junk tokens, so add them to the stopwords\n",
        "STOPWORDS = set(nltk.corpus.stopwords.words(\"english\") + [\n",
        "    \".\",\n",
        "    \",\",\n",
        "    \"<\",\n",
        "    \">\",\n",
        "    \"'s\",\n",
        "    \"''\",\n",
        "    \"``\",\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "def delete_headline(text: str) -> str:\n",
        "  '''\n",
        "  NLTK's sentence tokenizer includes the headline in the first sentence\n",
        "  if we don't manually exlude it.\n",
        "  '''\n",
        "  lines = text.split(\"\\n\")\n",
        "  return \"\\n\".join(lines[1:]) if len(lines) > 1 else text\n",
        "\n",
        "\n",
        "def global_freq(A: np.ndarray) -> np.ndarray:\n",
        "  '''Sum the columns of the term-frequency index to get term global frequencies'''\n",
        "  gf = np.zeros(A.shape[0])\n",
        "  for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "      gf[i] += A[i, j]\n",
        "  return gf\n",
        "\n",
        "\n",
        "def entropy(A: np.ndarray, gf: np.ndarray) -> np.ndarray:\n",
        "  '''Compute the term entropy'''\n",
        "  g = np.zeros(A.shape[0])\n",
        "  logN = np.log(A.shape[1])\n",
        "  for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        p_ij = A[i, j] / gf[i]\n",
        "        g[i] += p_ij * np.log(p_ij) if p_ij != 0 else 0\n",
        "    g[i] = 1 + g[i] / logN\n",
        "  return g\n",
        "\n",
        "\n",
        "def get_raw_tokens(file) -> List[str]:\n",
        "  '''\n",
        "  The raw NLTK documents contain a few HTML entities, so we'll use BeautifulSoup\n",
        "  to decode them, then apply the NLTK word tokenizer. Skip the headline.\n",
        "  '''\n",
        "  raw = BeautifulSoup(delete_headline(reuters.raw(file)), \"html.parser\").get_text()\n",
        "  return [t.casefold() for t in nltk.word_tokenize(raw) if t.casefold() not in STOPWORDS]\n",
        "\n",
        "\n",
        "def get_vocabulary(file) -> Set[str]:\n",
        "  '''\n",
        "  Returns the set of stemmed terms in the specified Reuters article (excluding headline).\n",
        "  '''\n",
        "  vocab: Set[str] = set()\n",
        "  tokens = get_raw_tokens(file)\n",
        "  stemmed = [STEMMER.stem(t.casefold()) for t in tokens]\n",
        "  return set(stemmed)\n",
        "\n",
        "\n",
        "file_ids = reuters.fileids()\n",
        "train_files = [id for id in file_ids if id.startswith(\"train\")][:500]\n",
        "\n",
        "vocab: Set[str] = set()\n",
        "\n",
        "for file in train_files:\n",
        "    vocab.update(get_vocabulary(file))\n",
        "\n",
        "ndocs = len(train_files)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"{ndocs} articles   {vocab_size} vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfucM66kMi9"
      },
      "source": [
        "It will also be handy to have mappings back and forth between each term (as a string) and the term's row in term frequency matrix. Let's build those up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ueo9hlKtkdV6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1624/3218619406.py:63: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  raw = BeautifulSoup(delete_headline(reuters.raw(file)), \"html.parser\").get_text()\n"
          ]
        }
      ],
      "source": [
        "vocab_map: Dict[str, int] = dict()\n",
        "rev_map: List[str] = [''] * vocab_size\n",
        "for i, t in enumerate(vocab):\n",
        "    vocab_map[t] = i\n",
        "    rev_map[i] = t\n",
        "\n",
        "index = np.zeros((vocab_size, ndocs))\n",
        "for col, id in enumerate(train_files):\n",
        "    tokens = get_raw_tokens(id)\n",
        "    stemmed = [STEMMER.stem(t) for t in tokens]\n",
        "    for term in stemmed:\n",
        "        index[ vocab_map[term], col ] += 1\n",
        "\n",
        "gf = global_freq(index)\n",
        "g = entropy(index, gf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6275,)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "g.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC7S59ADmq89"
      },
      "source": [
        "Now we have the inputs we need to compute the term weights, so we can implement our summarization algorithm. But since we want to monitor our summarization process with whylogs, we'll need to do a little whylogs setup before we start summarizing.\n",
        "\n",
        "By default, whylogs uses a `TransientLogger` that produces a new profile for every `log()` call. For our example, it's nicer to aggregate all the logging into a singe profile. So we'll create a simple `PersistentLogger` to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TrF-XmKsn35M"
      },
      "outputs": [],
      "source": [
        "from whylogs.api.logger.logger import Logger\n",
        "from whylogs.core import DatasetProfile, DatasetSchema\n",
        "from whylogs.core.configs import SummaryConfig\n",
        "from whylogs.core.dataset_profile import logger as dp_logger  # because it doesn't like vectors\n",
        "from whylogs.core.preprocessing import ListView, PreprocessedColumn\n",
        "from whylogs.core.resolvers import MetricSpec, ResolverSpec, STANDARD_RESOLVER\n",
        "from whylogs.core.schema import DeclarativeSchema\n",
        "from whylogs.core.stubs import pd\n",
        "from whylogs.core.view.column_profile_view import ColumnProfileView\n",
        "from whylogs.experimental.extras.nlp_metric import BagOfWordsMetric\n",
        "\n",
        "class PersistentLogger(Logger):\n",
        "    def __init__(self, schema: Optional[DatasetSchema] = None):\n",
        "        super().__init__(schema)\n",
        "        self._current_profile = DatasetProfile(schema=self._schema)\n",
        "\n",
        "    def _get_matching_profiles(\n",
        "        self,\n",
        "        obj: Any = None,\n",
        "        *,\n",
        "        pandas: Optional[pd.DataFrame] = None,\n",
        "        row: Optional[Dict[str, Any]] = None,\n",
        "        schema: Optional[DatasetSchema] = None,\n",
        "    ) -> List[DatasetProfile]:\n",
        "        if schema and schema is not self._schema:\n",
        "            raise ValueError(\n",
        "                \"You cannot pass a DatasetSchema to an instance of PersistentLogger.log(),\"\n",
        "                \"because schema is set once when instantiated, please use TimedRollingLogger(schema) instead.\"\n",
        "            )\n",
        "        return [self._current_profile]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI_c9bxIqy30"
      },
      "source": [
        "We also need to attach the `BagOfWordsMetric` to the columns that represent our input articles and output summaries. We log each document as a list of its tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qndtdoXBrmtl"
      },
      "outputs": [],
      "source": [
        "from logging import ERROR\n",
        "dp_logger.setLevel(ERROR)\n",
        "\n",
        "resolvers = STANDARD_RESOLVER + [\n",
        "    ResolverSpec(\n",
        "        column_name = \"article_bow\",\n",
        "        metrics = [MetricSpec(BagOfWordsMetric)]\n",
        "    ),\n",
        "    ResolverSpec(\n",
        "        column_name = \"summary_bow\",\n",
        "        metrics = [MetricSpec(BagOfWordsMetric)]\n",
        "    )\n",
        "]\n",
        "schema = DeclarativeSchema(resolvers)\n",
        "why = PersistentLogger(schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['training/1',\n",
              " 'training/10',\n",
              " 'training/100',\n",
              " 'training/1000',\n",
              " 'training/10000',\n",
              " 'training/10002',\n",
              " 'training/10005',\n",
              " 'training/10008',\n",
              " 'training/10011',\n",
              " 'training/10014',\n",
              " 'training/10015',\n",
              " 'training/10018',\n",
              " 'training/10023',\n",
              " 'training/10025',\n",
              " 'training/10027',\n",
              " 'training/1003',\n",
              " 'training/10032',\n",
              " 'training/10035',\n",
              " 'training/10037',\n",
              " 'training/10038',\n",
              " 'training/10040',\n",
              " 'training/10041',\n",
              " 'training/10042',\n",
              " 'training/10043',\n",
              " 'training/10046',\n",
              " 'training/10048',\n",
              " 'training/10049',\n",
              " 'training/10050',\n",
              " 'training/10052',\n",
              " 'training/10053',\n",
              " 'training/10054',\n",
              " 'training/10057',\n",
              " 'training/10058',\n",
              " 'training/10061',\n",
              " 'training/10062',\n",
              " 'training/10064',\n",
              " 'training/10065',\n",
              " 'training/10066',\n",
              " 'training/10067',\n",
              " 'training/10068',\n",
              " 'training/1007',\n",
              " 'training/10071',\n",
              " 'training/10073',\n",
              " 'training/10074',\n",
              " 'training/10075',\n",
              " 'training/10076',\n",
              " 'training/10078',\n",
              " 'training/10079',\n",
              " 'training/1008',\n",
              " 'training/10080',\n",
              " 'training/10081',\n",
              " 'training/10083',\n",
              " 'training/10085',\n",
              " 'training/10086',\n",
              " 'training/10088',\n",
              " 'training/10089',\n",
              " 'training/1009',\n",
              " 'training/10090',\n",
              " 'training/10091',\n",
              " 'training/10092',\n",
              " 'training/10094',\n",
              " 'training/10095',\n",
              " 'training/10096',\n",
              " 'training/10097',\n",
              " 'training/10098',\n",
              " 'training/10099',\n",
              " 'training/101',\n",
              " 'training/1010',\n",
              " 'training/10100',\n",
              " 'training/10102',\n",
              " 'training/10105',\n",
              " 'training/10106',\n",
              " 'training/10107',\n",
              " 'training/10108',\n",
              " 'training/10109',\n",
              " 'training/1011',\n",
              " 'training/10110',\n",
              " 'training/10112',\n",
              " 'training/10114',\n",
              " 'training/10115',\n",
              " 'training/10119',\n",
              " 'training/10120',\n",
              " 'training/10122',\n",
              " 'training/10124',\n",
              " 'training/10128',\n",
              " 'training/10129',\n",
              " 'training/1013',\n",
              " 'training/10130',\n",
              " 'training/10135',\n",
              " 'training/10136',\n",
              " 'training/10139',\n",
              " 'training/10140',\n",
              " 'training/10146',\n",
              " 'training/1015',\n",
              " 'training/10151',\n",
              " 'training/10153',\n",
              " 'training/10154',\n",
              " 'training/10155',\n",
              " 'training/10156',\n",
              " 'training/10158',\n",
              " 'training/1016',\n",
              " 'training/10162',\n",
              " 'training/10163',\n",
              " 'training/10165',\n",
              " 'training/10168',\n",
              " 'training/1017',\n",
              " 'training/10171',\n",
              " 'training/10172',\n",
              " 'training/10173',\n",
              " 'training/10174',\n",
              " 'training/10175',\n",
              " 'training/10176',\n",
              " 'training/10177',\n",
              " 'training/10178',\n",
              " 'training/10180',\n",
              " 'training/10181',\n",
              " 'training/10183',\n",
              " 'training/10185',\n",
              " 'training/10186',\n",
              " 'training/1019',\n",
              " 'training/10190',\n",
              " 'training/10191',\n",
              " 'training/10192',\n",
              " 'training/10196',\n",
              " 'training/10197',\n",
              " 'training/102',\n",
              " 'training/1020',\n",
              " 'training/10200',\n",
              " 'training/10201',\n",
              " 'training/10207',\n",
              " 'training/10208',\n",
              " 'training/10209',\n",
              " 'training/10211',\n",
              " 'training/10213',\n",
              " 'training/10214',\n",
              " 'training/10215',\n",
              " 'training/10216',\n",
              " 'training/10218',\n",
              " 'training/10219',\n",
              " 'training/1022',\n",
              " 'training/10222',\n",
              " 'training/10224',\n",
              " 'training/10225',\n",
              " 'training/10227',\n",
              " 'training/10228',\n",
              " 'training/1023',\n",
              " 'training/10230',\n",
              " 'training/10231',\n",
              " 'training/10233',\n",
              " 'training/10235',\n",
              " 'training/10237',\n",
              " 'training/10238',\n",
              " 'training/1024',\n",
              " 'training/10240',\n",
              " 'training/10242',\n",
              " 'training/10243',\n",
              " 'training/10245',\n",
              " 'training/10246',\n",
              " 'training/10253',\n",
              " 'training/10254',\n",
              " 'training/10255',\n",
              " 'training/10258',\n",
              " 'training/1026',\n",
              " 'training/10260',\n",
              " 'training/10261',\n",
              " 'training/10262',\n",
              " 'training/10263',\n",
              " 'training/10264',\n",
              " 'training/10265',\n",
              " 'training/10268',\n",
              " 'training/10272',\n",
              " 'training/10275',\n",
              " 'training/10277',\n",
              " 'training/10280',\n",
              " 'training/10281',\n",
              " 'training/10282',\n",
              " 'training/10283',\n",
              " 'training/10284',\n",
              " 'training/10285',\n",
              " 'training/10289',\n",
              " 'training/1029',\n",
              " 'training/10291',\n",
              " 'training/10292',\n",
              " 'training/10294',\n",
              " 'training/10295',\n",
              " 'training/10297',\n",
              " 'training/10299',\n",
              " 'training/103',\n",
              " 'training/1030',\n",
              " 'training/10300',\n",
              " 'training/10302',\n",
              " 'training/10305',\n",
              " 'training/10306',\n",
              " 'training/10308',\n",
              " 'training/10310',\n",
              " 'training/10311',\n",
              " 'training/10313',\n",
              " 'training/10314',\n",
              " 'training/10315',\n",
              " 'training/10318',\n",
              " 'training/10319',\n",
              " 'training/1032',\n",
              " 'training/10321',\n",
              " 'training/10322',\n",
              " 'training/10323',\n",
              " 'training/10324',\n",
              " 'training/10328',\n",
              " 'training/1033',\n",
              " 'training/10330',\n",
              " 'training/10332',\n",
              " 'training/10334',\n",
              " 'training/10335',\n",
              " 'training/10337',\n",
              " 'training/10338',\n",
              " 'training/10339',\n",
              " 'training/10340',\n",
              " 'training/10341',\n",
              " 'training/10342',\n",
              " 'training/10344',\n",
              " 'training/10347',\n",
              " 'training/10348',\n",
              " 'training/10349',\n",
              " 'training/1035',\n",
              " 'training/10352',\n",
              " 'training/10354',\n",
              " 'training/10355',\n",
              " 'training/10356',\n",
              " 'training/10357',\n",
              " 'training/10358',\n",
              " 'training/10359',\n",
              " 'training/1036',\n",
              " 'training/10361',\n",
              " 'training/10362',\n",
              " 'training/10364',\n",
              " 'training/10367',\n",
              " 'training/10368',\n",
              " 'training/10370',\n",
              " 'training/10371',\n",
              " 'training/10372',\n",
              " 'training/10373',\n",
              " 'training/10375',\n",
              " 'training/10376',\n",
              " 'training/10377',\n",
              " 'training/10381',\n",
              " 'training/10382',\n",
              " 'training/10384',\n",
              " 'training/10385',\n",
              " 'training/10387',\n",
              " 'training/10388',\n",
              " 'training/1039',\n",
              " 'training/10390',\n",
              " 'training/10391',\n",
              " 'training/10393',\n",
              " 'training/10394',\n",
              " 'training/10395',\n",
              " 'training/10397',\n",
              " 'training/10398',\n",
              " 'training/10399',\n",
              " 'training/104',\n",
              " 'training/1040',\n",
              " 'training/10401',\n",
              " 'training/10402',\n",
              " 'training/10403',\n",
              " 'training/10404',\n",
              " 'training/10406',\n",
              " 'training/10411',\n",
              " 'training/10412',\n",
              " 'training/10414',\n",
              " 'training/10416',\n",
              " 'training/10418',\n",
              " 'training/10419',\n",
              " 'training/10423',\n",
              " 'training/10427',\n",
              " 'training/10429',\n",
              " 'training/10430',\n",
              " 'training/10431',\n",
              " 'training/10432',\n",
              " 'training/10434',\n",
              " 'training/10435',\n",
              " 'training/10436',\n",
              " 'training/10439',\n",
              " 'training/1044',\n",
              " 'training/10440',\n",
              " 'training/10442',\n",
              " 'training/10444',\n",
              " 'training/10445',\n",
              " 'training/10446',\n",
              " 'training/10447',\n",
              " 'training/10449',\n",
              " 'training/10452',\n",
              " 'training/10453',\n",
              " 'training/10454',\n",
              " 'training/10455',\n",
              " 'training/10458',\n",
              " 'training/10459',\n",
              " 'training/10462',\n",
              " 'training/10464',\n",
              " 'training/10467',\n",
              " 'training/1047',\n",
              " 'training/10470',\n",
              " 'training/10471',\n",
              " 'training/10472',\n",
              " 'training/10473',\n",
              " 'training/10475',\n",
              " 'training/10478',\n",
              " 'training/10484',\n",
              " 'training/10485',\n",
              " 'training/10487',\n",
              " 'training/10489',\n",
              " 'training/1049',\n",
              " 'training/10491',\n",
              " 'training/10494',\n",
              " 'training/10498',\n",
              " 'training/10499',\n",
              " 'training/105',\n",
              " 'training/1050',\n",
              " 'training/10500',\n",
              " 'training/10503',\n",
              " 'training/10505',\n",
              " 'training/10506',\n",
              " 'training/10508',\n",
              " 'training/10509',\n",
              " 'training/10515',\n",
              " 'training/10518',\n",
              " 'training/10519',\n",
              " 'training/1052',\n",
              " 'training/10520',\n",
              " 'training/10521',\n",
              " 'training/10524',\n",
              " 'training/10527',\n",
              " 'training/10529',\n",
              " 'training/10530',\n",
              " 'training/10531',\n",
              " 'training/10532',\n",
              " 'training/10534',\n",
              " 'training/10536',\n",
              " 'training/10537',\n",
              " 'training/10538',\n",
              " 'training/10539',\n",
              " 'training/10541',\n",
              " 'training/10542',\n",
              " 'training/10544',\n",
              " 'training/10545',\n",
              " 'training/10546',\n",
              " 'training/10547',\n",
              " 'training/10548',\n",
              " 'training/10551',\n",
              " 'training/10553',\n",
              " 'training/10555',\n",
              " 'training/10556',\n",
              " 'training/10559',\n",
              " 'training/10561',\n",
              " 'training/10563',\n",
              " 'training/10564',\n",
              " 'training/10565',\n",
              " 'training/10566',\n",
              " 'training/10567',\n",
              " 'training/10569',\n",
              " 'training/1057',\n",
              " 'training/10573',\n",
              " 'training/10576',\n",
              " 'training/1058',\n",
              " 'training/10581',\n",
              " 'training/10582',\n",
              " 'training/10584',\n",
              " 'training/10585',\n",
              " 'training/10586',\n",
              " 'training/10587',\n",
              " 'training/10588',\n",
              " 'training/1059',\n",
              " 'training/10590',\n",
              " 'training/10592',\n",
              " 'training/10593',\n",
              " 'training/10594',\n",
              " 'training/10596',\n",
              " 'training/10597',\n",
              " 'training/10598',\n",
              " 'training/10599',\n",
              " 'training/106',\n",
              " 'training/10600',\n",
              " 'training/10601',\n",
              " 'training/10602',\n",
              " 'training/10604',\n",
              " 'training/10606',\n",
              " 'training/10608',\n",
              " 'training/10609',\n",
              " 'training/10610',\n",
              " 'training/10611',\n",
              " 'training/10612',\n",
              " 'training/10613',\n",
              " 'training/10615',\n",
              " 'training/10616',\n",
              " 'training/10617',\n",
              " 'training/10618',\n",
              " 'training/10619',\n",
              " 'training/10620',\n",
              " 'training/10621',\n",
              " 'training/10622',\n",
              " 'training/10623',\n",
              " 'training/10625',\n",
              " 'training/10627',\n",
              " 'training/1063',\n",
              " 'training/10632',\n",
              " 'training/10633',\n",
              " 'training/10634',\n",
              " 'training/10636',\n",
              " 'training/10638',\n",
              " 'training/10640',\n",
              " 'training/10641',\n",
              " 'training/10642',\n",
              " 'training/10643',\n",
              " 'training/10644',\n",
              " 'training/10648',\n",
              " 'training/1065',\n",
              " 'training/10650',\n",
              " 'training/10651',\n",
              " 'training/10652',\n",
              " 'training/10653',\n",
              " 'training/10654',\n",
              " 'training/10657',\n",
              " 'training/10658',\n",
              " 'training/10659',\n",
              " 'training/1066',\n",
              " 'training/10660',\n",
              " 'training/10661',\n",
              " 'training/10662',\n",
              " 'training/10665',\n",
              " 'training/10669',\n",
              " 'training/1067',\n",
              " 'training/10674',\n",
              " 'training/10676',\n",
              " 'training/10678',\n",
              " 'training/10679',\n",
              " 'training/1068',\n",
              " 'training/10681',\n",
              " 'training/10682',\n",
              " 'training/10684',\n",
              " 'training/10685',\n",
              " 'training/10687',\n",
              " 'training/10688',\n",
              " 'training/10689',\n",
              " 'training/1069',\n",
              " 'training/10692',\n",
              " 'training/10693',\n",
              " 'training/10694',\n",
              " 'training/10695',\n",
              " 'training/10696',\n",
              " 'training/10699',\n",
              " 'training/1070',\n",
              " 'training/10701',\n",
              " 'training/10702',\n",
              " 'training/10703',\n",
              " 'training/10704',\n",
              " 'training/10705',\n",
              " 'training/10708',\n",
              " 'training/10709',\n",
              " 'training/1071',\n",
              " 'training/10710',\n",
              " 'training/10712',\n",
              " 'training/10714',\n",
              " 'training/10715',\n",
              " 'training/10717',\n",
              " 'training/10718',\n",
              " 'training/1072',\n",
              " 'training/10720',\n",
              " 'training/10721',\n",
              " 'training/10724',\n",
              " 'training/10729',\n",
              " 'training/10731',\n",
              " 'training/10734',\n",
              " 'training/10736',\n",
              " 'training/10737',\n",
              " 'training/10738',\n",
              " 'training/1074',\n",
              " 'training/10742',\n",
              " 'training/10743',\n",
              " 'training/10744',\n",
              " 'training/10746',\n",
              " 'training/10748',\n",
              " 'training/10749',\n",
              " 'training/10750',\n",
              " 'training/10751',\n",
              " 'training/10752',\n",
              " 'training/10754',\n",
              " 'training/10755',\n",
              " 'training/10756',\n",
              " 'training/10757',\n",
              " 'training/10758',\n",
              " 'training/1076',\n",
              " 'training/10760',\n",
              " 'training/10762',\n",
              " 'training/10766',\n",
              " 'training/10767',\n",
              " 'training/10768',\n",
              " 'training/10769',\n",
              " 'training/1077',\n",
              " 'training/10770',\n",
              " 'training/10771',\n",
              " 'training/10773',\n",
              " 'training/10774']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNm95G61tFtf"
      },
      "source": [
        "Now we're finally ready to do some summarization! We'll compute the log entropy weighted term vector for each article as a whole, then use NLTK's sentence tokenizer to split it into sentences. The first sentence that contains the word with the highest weight in the document will be our summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8keC6Y-QtrpD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "document tokens: ['showers', 'continued', 'throughout', 'week', 'bahia', 'cocoa', 'zone', 'alleviating', 'drought', 'since', 'early', 'january', 'improving', 'prospects', 'coming', 'temporao', 'although', 'normal', 'humidity', 'levels', 'restored', 'comissaria', 'smith', 'said', 'weekly', 'review', 'dry', 'period', 'means', 'temporao', 'late', 'year', 'arrivals', 'week', 'ended', 'february', '22', '155,221', 'bags', '60', 'kilos', 'making', 'cumulative', 'total', 'season', '5.93', 'mln', '5.81', 'stage', 'last', 'year', 'seems', 'cocoa', 'delivered', 'earlier', 'consignment', 'included', 'arrivals', 'figures', 'comissaria', 'smith', 'said', 'still', 'doubt', 'much', 'old', 'crop', 'cocoa', 'still', 'available', 'harvesting', 'practically', 'come', 'end', 'total', 'bahia', 'crop', 'estimates', 'around', '6.4', 'mln', 'bags', 'sales', 'standing', 'almost', '6.2', 'mln', 'hundred', 'thousand', 'bags', 'still', 'hands', 'farmers', 'middlemen', 'exporters', 'processors', 'doubts', 'much', 'cocoa', 'would', 'fit', 'export', 'shippers', 'experiencing', 'dificulties', 'obtaining', '+bahia', 'superior+', 'certificates', 'view', 'lower', 'quality', 'recent', 'weeks', 'farmers', 'sold', 'good', 'part', 'cocoa', 'held', 'consignment', 'comissaria', 'smith', 'said', 'spot', 'bean', 'prices', 'rose', '340', '350', 'cruzados', 'per', 'arroba', '15', 'kilos', 'bean', 'shippers', 'reluctant', 'offer', 'nearby', 'shipment', 'limited', 'sales', 'booked', 'march', 'shipment', '1,750', '1,780', 'dlrs', 'per', 'tonne', 'ports', 'named', 'new', 'crop', 'sales', 'also', 'light', 'open', 'ports', 'june/july', 'going', '1,850', '1,880', 'dlrs', '35', '45', 'dlrs', 'new', 'york', 'july', 'aug/sept', '1,870', '1,875', '1,880', 'dlrs', 'per', 'tonne', 'fob', 'routine', 'sales', 'butter', 'made', 'march/april', 'sold', '4,340', '4,345', '4,350', 'dlrs', 'april/may', 'butter', 'went', '2.27', 'times', 'new', 'york', 'may', 'june/july', '4,400', '4,415', 'dlrs', 'aug/sept', '4,351', '4,450', 'dlrs', '2.27', '2.28', 'times', 'new', 'york', 'sept', 'oct/dec', '4,480', 'dlrs', '2.27', 'times', 'new', 'york', 'dec', 'comissaria', 'smith', 'said', 'destinations', 'u.s.', 'covertible', 'currency', 'areas', 'uruguay', 'open', 'ports', 'cake', 'sales', 'registered', '785', '995', 'dlrs', 'march/april', '785', 'dlrs', 'may', '753', 'dlrs', 'aug', '0.39', 'times', 'new', 'york', 'dec', 'oct/dec', 'buyers', 'u.s.', 'argentina', 'uruguay', 'convertible', 'currency', 'areas', 'liquor', 'sales', 'limited', 'march/april', 'selling', '2,325', '2,380', 'dlrs', 'june/july', '2,375', 'dlrs', '1.25', 'times', 'new', 'york', 'july', 'aug/sept', '2,400', 'dlrs', '1.25', 'times', 'new', 'york', 'sept', 'oct/dec', '1.25', 'times', 'new', 'york', 'dec', 'comissaria', 'smith', 'said', 'total', 'bahia', 'sales', 'currently', 'estimated', '6.13', 'mln', 'bags', '1986/87', 'crop', '1.06', 'mln', 'bags', '1987/88', 'crop', 'final', 'figures', 'period', 'february', '28', 'expected', 'published', 'brazilian', 'cocoa', 'trade', 'commission', 'carnival', 'ends', 'midday', 'february', '27']\n",
            "summary tokens: ['showers', 'continued', 'throughout', 'week', 'bahia', 'cocoa', 'zone', 'alleviating', 'drought', 'since', 'early', 'january', 'improving', 'prospects', 'coming', 'temporao', 'although', 'normal', 'humidity', 'levels', 'restored', 'comissaria', 'smith', 'said', 'weekly', 'review']\n",
            "document tokens: ['computer', 'terminal', 'systems', 'inc', 'said', 'completed', 'sale', '200,000', 'shares', 'common', 'stock', 'warrants', 'acquire', 'additional', 'one', 'mln', 'shares', 'sedio', 'n.v.', 'lugano', 'switzerland', '50,000', 'dlrs', 'company', 'said', 'warrants', 'exercisable', 'five', 'years', 'purchase', 'price', '.125', 'dlrs', 'per', 'share', 'computer', 'terminal', 'said', 'sedio', 'also', 'right', 'buy', 'additional', 'shares', 'increase', 'total', 'holdings', '40', 'pct', 'computer', 'terminal', 'outstanding', 'common', 'stock', 'certain', 'circumstances', 'involving', 'change', 'control', 'company', 'company', 'said', 'conditions', 'occur', 'warrants', 'would', 'exercisable', 'price', 'equal', '75', 'pct', 'common', \"stock's\", 'market', 'price', 'time', 'exceed', '1.50', 'dlrs', 'per', 'share', 'computer', 'terminal', 'also', 'said', 'sold', 'technolgy', 'rights', 'dot', 'matrix', 'impact', 'technology', 'including', 'future', 'improvements', 'woodco', 'inc', 'houston', 'tex', '200,000', 'dlrs', 'said', 'would', 'continue', 'exclusive', 'worldwide', 'licensee', 'technology', 'woodco', 'company', 'said', 'moves', 'part', 'reorganization', 'plan', 'would', 'help', 'pay', 'current', 'operation', 'costs', 'ensure', 'product', 'delivery', 'computer', 'terminal', 'makes', 'computer', 'generated', 'labels', 'forms', 'tags', 'ticket', 'printers', 'terminals']\n",
            "summary tokens: ['computer', 'terminal', 'systems', 'inc', 'said', 'completed', 'sale', '200,000', 'shares', 'common', 'stock', 'warrants', 'acquire', 'additional', 'one', 'mln', 'shares', 'sedio', 'n.v.', 'lugano', 'switzerland', '50,000', 'dlrs']\n",
            "document tokens: ['new', 'zealand', 'trading', 'bank', 'seasonally', 'adjusted', 'deposit', 'growth', 'rose', '2.6', 'pct', 'january', 'compared', 'rise', '9.4', 'pct', 'december', 'reserve', 'bank', 'said', 'year-on-year', 'total', 'deposits', 'rose', '30.6', 'pct', 'compared', '26.3', 'pct', 'increase', 'december', 'year', '34.5', 'pct', 'rise', 'year', 'ago', 'period', 'said', 'weekly', 'statistical', 'release', 'total', 'deposits', 'rose', '17.18', 'billion', 'n.z', 'dlrs', 'january', 'compared', '16.74', 'billion', 'december', '13.16', 'billion', 'january', '1986']\n",
            "summary tokens: ['new', 'zealand', 'trading', 'bank', 'seasonally', 'adjusted', 'deposit', 'growth', 'rose', '2.6', 'pct', 'january', 'compared', 'rise', '9.4', 'pct', 'december', 'reserve', 'bank', 'said']\n",
            "document tokens: ['viacom', 'international', 'inc', 'said', 'national', 'amusements', 'inc', 'raised', 'value', 'offer', 'viacom', 'publicly', 'held', 'stock', 'company', 'said', 'special', 'committee', 'board', 'plans', 'meet', 'later', 'today', 'consider', 'offer', 'one', 'submitted', 'march', 'one', 'mcv', 'holdings', 'inc', 'spokeswoman', 'unable', 'say', 'committee', 'met', 'planned', 'yesterday', 'viacom', 'said', 'national', 'amusements', \"'\", 'arsenal', 'holdings', 'inc', 'subsidiary', 'raised', 'amount', 'cash', 'offering', 'viacom', 'share', '75', 'cts', '42.75', 'dlrs', 'value', 'fraction', 'share', 'exchangeable', 'arsenal', 'holdings', 'preferred', 'included', 'raised', '25', 'cts', '7.75', 'dlrs', 'national', 'amusements', 'already', 'owns', '19.6', 'pct', 'viacom', 'stock']\n",
            "summary tokens: ['viacom', 'international', 'inc', 'said', 'national', 'amusements', 'inc', 'raised', 'value', 'offer', 'viacom', 'publicly', 'held', 'stock']\n",
            "document tokens: ['rogers', 'corp', 'said', 'first', 'quarter', 'earnings', 'significantly', 'earnings', '114,000', 'dlrs', 'four', 'cts', 'share', 'quarter', 'last', 'year', 'company', 'said', 'expects', 'revenues', 'first', 'quarter', 'somewhat', 'higher', 'revenues', '32.9', 'mln', 'dlrs', 'posted', 'year-ago', 'quarter', 'rogers', 'said', 'reached', 'agreement', 'sale', 'molded', 'switch', 'circuit', 'product', 'line', 'major', 'supplier', 'sale', 'terms', 'disclosed', 'completed', 'early', 'second', 'quarter', 'rogers', 'said']\n",
            "summary tokens: ['rogers', 'corp', 'said', 'first', 'quarter', 'earnings', 'significantly', 'earnings', '114,000', 'dlrs', 'four', 'cts', 'share', 'quarter', 'last', 'year']\n"
          ]
        }
      ],
      "source": [
        "profile = None\n",
        "for file in train_files[:5]:\n",
        "    raw = BeautifulSoup(reuters.raw(file), 'html.parser').get_text()\n",
        "    # print(raw.split('\\n')[0])   # print article headline\n",
        "    # print(raw)  # print the whole input article\n",
        "    raw = delete_headline(raw)\n",
        "    tokens = [t.casefold() for t in nltk.word_tokenize(raw) if t.casefold() not in STOPWORDS]\n",
        "    stemmed = [STEMMER.stem(t) for t in tokens]\n",
        "    doc_vec = np.zeros(vocab_size)\n",
        "    for term in stemmed:\n",
        "        doc_vec[ vocab_map[term] ] += 1\n",
        "    max_weight = -1\n",
        "    max_term = \"\"\n",
        "    for i in range(vocab_size):\n",
        "        doc_vec[i] = g[i] * np.log(doc_vec[i] + 1.0)\n",
        "        if doc_vec[i] > max_weight:\n",
        "            max_weight = doc_vec[i]\n",
        "            max_term = rev_map[i]\n",
        "    sentences = nltk.sent_tokenize(raw)\n",
        "    max_sentence = \"\"\n",
        "    for sentence in sentences:\n",
        "        tokenized = [t.casefold() for t in nltk.word_tokenize(sentence) if t.casefold() not in STOPWORDS]\n",
        "        stemmed = [STEMMER.stem(t) for t in tokenized]\n",
        "        if max_term in stemmed:\n",
        "            max_sentence = sentence\n",
        "            print(\"document tokens:\", tokens)\n",
        "            print(\"summary tokens:\", tokenized)\n",
        "            profile = why.log(obj={\"article_bow\": tokens, \"summary_bow\": tokenized})\n",
        "            break\n",
        "    # max_sentence = max_sentence.replace(\"\\n\", \" \")\n",
        "    # print(f\"{max_weight} {max_term}:   {max_sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-9ja982gCP"
      },
      "source": [
        "We've logged the full articles as the `article_bow` column and the summaries as the `summary_bow` column. Now let's grab the profile from the logger and take a look at it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcqenmOo21FB",
        "outputId": "d51d9137-8a81-4169-99af-a943289c3b3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "article_bow:\n",
            "    nlp_bow/doc_length:counts/n: 500\n",
            "    nlp_bow/doc_length:distribution/mean: 88.38000000000004\n",
            "    nlp_bow/doc_length:distribution/stddev: 89.40470907065252\n",
            "    nlp_bow/doc_length:distribution/max: 504.0\n",
            "    nlp_bow/doc_length:distribution/min: 1.0\n",
            "    nlp_bow/doc_length:distribution/median: 59.0\n",
            "    nlp_bow/term_length:counts/n: 44190\n",
            "    nlp_bow/term_length:distribution/mean: 5.906223127404392\n",
            "    nlp_bow/term_length:distribution/stddev: 2.5306350762162584\n",
            "    nlp_bow/term_length:distribution/max: 24.0\n",
            "    nlp_bow/term_length:distribution/min: 1.0\n",
            "    nlp_bow/term_length:distribution/median: 6.0\n",
            "    frequent terms: ['said', 'mln', 'dlrs', 'pct', 'vs', 'billion', 'year', 'cts', 'would', 'u.s.']\n",
            "\n",
            "summary_bow:\n",
            "    nlp_bow/doc_length:counts/n: 500\n",
            "    nlp_bow/doc_length:distribution/mean: 21.554000000000002\n",
            "    nlp_bow/doc_length:distribution/stddev: 14.143095074153782\n",
            "    nlp_bow/doc_length:distribution/max: 176.0\n",
            "    nlp_bow/doc_length:distribution/min: 1.0\n",
            "    nlp_bow/doc_length:distribution/median: 18.0\n",
            "    nlp_bow/term_length:counts/n: 10777\n",
            "    nlp_bow/term_length:distribution/mean: 5.419690080727475\n",
            "    nlp_bow/term_length:distribution/stddev: 2.5998033619617535\n",
            "    nlp_bow/term_length:distribution/max: 21.0\n",
            "    nlp_bow/term_length:distribution/min: 1.0\n",
            "    nlp_bow/term_length:distribution/median: 5.0\n",
            "    frequent terms: ['vs', 'mln', 'said', 'cts', 'loss', 'net', 'dlrs', 'shr', 'inc', 'billion']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def dump_summary(view: ColumnProfileView) -> None:\n",
        "    summary = view.to_summary_dict()\n",
        "    keys = [\n",
        "        \"nlp_bow/doc_length:counts/n\",\n",
        "        \"nlp_bow/doc_length:distribution/mean\",\n",
        "        \"nlp_bow/doc_length:distribution/stddev\",\n",
        "        \"nlp_bow/doc_length:distribution/max\",\n",
        "        \"nlp_bow/doc_length:distribution/min\",\n",
        "        \"nlp_bow/doc_length:distribution/median\",\n",
        "\n",
        "        \"nlp_bow/term_length:counts/n\",\n",
        "        \"nlp_bow/term_length:distribution/mean\",\n",
        "        \"nlp_bow/term_length:distribution/stddev\",\n",
        "        \"nlp_bow/term_length:distribution/max\",\n",
        "        \"nlp_bow/term_length:distribution/min\",\n",
        "        \"nlp_bow/term_length:distribution/median\",\n",
        "    ]\n",
        "    for key in keys:\n",
        "        print(f\"    {key}: {summary[key]}\")\n",
        "    print(f\"    frequent terms: {[t.value for t in summary['nlp_bow/frequent_terms:frequent_items/frequent_strings'][:10]]}\")\n",
        "\n",
        "\n",
        "view = profile.view()\n",
        "columns = view.get_columns()\n",
        "for col_name, col_view in columns.items():\n",
        "    print(f\"{col_name}:\")\n",
        "    dump_summary(col_view)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR2FQYlN3Hom"
      },
      "source": [
        "As expected, we see that the summary documents are shorter than the original articles. We also see some differences and overlap in the most frequent words in the whole articles and the summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KFTk_cvX_kz",
        "outputId": "dcf01e96-8e37-4f5f-d5e4-94a5e3594b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original_bow:\n",
            "    nlp_bow/doc_length:counts/n: 0\n",
            "    nlp_bow/doc_length:distribution/mean: 0.0\n",
            "    nlp_bow/doc_length:distribution/stddev: 0.0\n",
            "    nlp_bow/doc_length:distribution/max: nan\n",
            "    nlp_bow/doc_length:distribution/min: nan\n",
            "    nlp_bow/doc_length:distribution/median: None\n",
            "    nlp_bow/term_length:counts/n: 0\n",
            "    nlp_bow/term_length:distribution/mean: 0.0\n",
            "    nlp_bow/term_length:distribution/stddev: 0.0\n",
            "    nlp_bow/term_length:distribution/max: nan\n",
            "    nlp_bow/term_length:distribution/min: nan\n",
            "    nlp_bow/term_length:distribution/median: None\n",
            "    frequent terms: []\n",
            "\n",
            "split_bow:\n",
            "    nlp_bow/doc_length:counts/n: 4545\n",
            "    nlp_bow/doc_length:distribution/mean: 16.67216721672163\n",
            "    nlp_bow/doc_length:distribution/stddev: 13.849889480931045\n",
            "    nlp_bow/doc_length:distribution/max: 207.0\n",
            "    nlp_bow/doc_length:distribution/min: 0.0\n",
            "    nlp_bow/doc_length:distribution/median: 15.0\n",
            "    nlp_bow/term_length:counts/n: 75775\n",
            "    nlp_bow/term_length:distribution/mean: 4.389495216100277\n",
            "    nlp_bow/term_length:distribution/stddev: 2.7034094987711406\n",
            "    nlp_bow/term_length:distribution/max: 24.0\n",
            "    nlp_bow/term_length:distribution/min: 1.0\n",
            "    nlp_bow/term_length:distribution/median: 4.0\n",
            "    frequent terms: ['the', '.', ',', 'to', 'of', 'in', 'said', 'and', 'a', 'mln']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "resolvers = STANDARD_RESOLVER + [\n",
        "    ResolverSpec(\n",
        "        column_name = \"original_bow\",\n",
        "        metrics = [MetricSpec(BagOfWordsMetric)]\n",
        "    ),\n",
        "    ResolverSpec(\n",
        "        column_name = \"split_bow\",\n",
        "        metrics = [MetricSpec(BagOfWordsMetric)]\n",
        "    )\n",
        "]\n",
        "schema = DeclarativeSchema(resolvers)\n",
        "why = PersistentLogger(schema=schema)\n",
        "\n",
        "import random\n",
        "\n",
        "profile = None\n",
        "for file in train_files:\n",
        "    raw = BeautifulSoup(reuters.raw(file), 'html.parser').get_text()\n",
        "    raw = delete_headline(raw)\n",
        "    sentences = nltk.sent_tokenize(raw)\n",
        "    for sentence in sentences:\n",
        "      tokens = [t.casefold() for t in nltk.word_tokenize(sentence)]\n",
        "      why.log(obj={\"original_bow\": np.array(tokens)})\n",
        "      phrases = sentence.split(\",\")\n",
        "      if len(phrases) > 1:\n",
        "        index = random.randint(0, len(phrases))\n",
        "        left = [t.casefold() for t in nltk.word_tokenize(\", \".join(phrases[:index]) + \".\")]\n",
        "        right = [t.casefold() for t in nltk.word_tokenize(\", \".join(phrases[index:]))]\n",
        "        why.log(obj={\"split_bow\": left})\n",
        "        profile = why.log(obj={\"split_bow\": right})\n",
        "      else:\n",
        "        profile = why.log(obj={\"split_bow\": tokens})\n",
        "\n",
        "view = profile.view()\n",
        "columns = view.get_columns()\n",
        "for col_name, col_view in columns.items():\n",
        "    print(f\"{col_name}:\")\n",
        "    dump_summary(col_view)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
