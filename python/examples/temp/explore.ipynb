{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scope\n",
    "\n",
    "- Estimate Accuracy for Binary Classification\n",
    "- Single Partition - Segments from a single column\n",
    "- External Visualization (Matplotlib,etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"df_with_predictions.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7777777777777778\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.4482758620689655\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_subsample_on_column(df, column, lower_pct=0.3 , upper_pct=1, classes = 'all'):\n",
    "    \"\"\"Subsample each class in a column to a random percentage of the total.\n",
    "\n",
    "    The percentage is sampled uniformly between lower_pct and upper_pct.\n",
    "    If classes is not 'all', then only subsample the classes in classes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to subsample.\n",
    "        column (str): The column to subsample on.\n",
    "        lower_pct (float): The lower bound of the percentage to subsample.\n",
    "        upper_pct (float): The upper bound of the percentage to subsample.\n",
    "        classes (list): The classes to subsample. If 'all', then subsample all classes.\n",
    "\n",
    "    \"\"\"\n",
    "    if classes == 'all':\n",
    "        class_names = df[column].unique()\n",
    "    elif isinstance(classes, list):\n",
    "        assert all([c in df[column].unique() for c in classes]), \"Classes must be in the column\"\n",
    "        class_names = classes\n",
    "    for c in class_names:\n",
    "        sub_df = df.loc[df[column]==c]\n",
    "        n = int(len(sub_df) * (lower_pct + (upper_pct - lower_pct) * np.random.random()))\n",
    "        # remove n rows from the class\n",
    "        df = df.loc[df[column] != c].append(sub_df.sample(n=n))\n",
    "    return df\n",
    "\n",
    "df_2 = random_subsample_on_column(df, 'group', lower_pct=0.3, upper_pct=1, classes = ['black','white'])\n",
    "\n",
    "df_2.groupby('group').size()\n",
    "for value,sub_value in zip(df.groupby('group').size(),df_2.groupby('group').size()):\n",
    "    print(sub_value/value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/s-nlp/roberta_toxicity_classifier\n",
    "\n",
    "import whylogs as why\n",
    "from whylogs.core.segmentation_partition import segment_on_column\n",
    "from whylogs.core.schema import DatasetSchema\n",
    "\n",
    "def log_with_metrics(df):\n",
    "    segment_column = \"group\"\n",
    "    segmented_schema = DatasetSchema(segments=segment_on_column(segment_column))\n",
    "    results = why.log_classification_metrics(\n",
    "        df,\n",
    "        target_column = \"output_toxicity\",\n",
    "        prediction_column = \"output_prediction\",\n",
    "        score_column=\"output_score\",\n",
    "        schema=segmented_schema,\n",
    "        log_full_data=True\n",
    "    )\n",
    "    return results\n",
    "results = log_with_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_real_accuracy(df):\n",
    "    metrics_df = df[['output_toxicity','output_prediction']]\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for index,row in metrics_df.iterrows():\n",
    "        \n",
    "        if row['output_toxicity'] == row['output_prediction']:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    acc = correct/(correct+incorrect)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_real_accuracy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perturbed = random_subsample_on_column(df, 'group', lower_pct=0.3, upper_pct=1)\n",
    "perturbed_results = log_with_metrics(df_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference accuracy: 0.9485530546623794\n",
      "Perturbed accuracy: 0.949671772428884\n"
     ]
    }
   ],
   "source": [
    "def estimate_accuracy(reference_results, target_results):\n",
    "    \n",
    "    if len(reference_results.partitions)> 1 and len(target_results.partitions)> 1:\n",
    "        warnings.warn(\"More than one partition found. Only the first partition will be used for the estimation.\")\n",
    "    if len(reference_results.partitions) != len(target_results.partitions):\n",
    "        raise ValueError(\"The number of partitions in the reference and target results must be the same.\")\n",
    "\n",
    "    reference_partition = reference_results.partitions[0]\n",
    "    target_partition = target_results.partitions[0]\n",
    "\n",
    "    segmented_column = reference_partition.name\n",
    "    if segmented_column != target_partition.name:\n",
    "        raise ValueError(\"The segmented columns in the reference and target results must be the same.\")\n",
    "\n",
    "    reference_segments = reference_results.segments_in_partition(reference_partition)\n",
    "    target_segments = target_results.segments_in_partition(target_partition)\n",
    "\n",
    "    if any([len(segment.key)>1 for segment in reference_segments]):\n",
    "        raise ValueError(\"Only single key segments are supported.\")\n",
    "    if any([len(segment.key)>1 for segment in target_segments]):\n",
    "        raise ValueError(\"Only single key segments are supported.\")\n",
    "\n",
    "\n",
    "    # make a set out of the keys in the reference segments\n",
    "    reference_keys = set([segment.key[0] for segment in reference_segments])\n",
    "    target_keys = set([segment.key[0] for segment in target_segments])\n",
    "    if reference_keys != target_keys:\n",
    "        raise ValueError(\"The keys in the reference and target segments must be the same.\")\n",
    "    reference_accuracies = {}\n",
    "    for reference_segment in reference_segments:\n",
    "        id = reference_segment.parent_id\n",
    "        reference_conf = reference_results._segments[id][reference_segment].model_performance_metrics.confusion_matrix.confusion_matrix\n",
    "        target_segment = next(segment for segment in target_segments if segment.key[0] == reference_segment.key[0])\n",
    "        tp = get_cell_from_confusion_matrix(reference_conf, (1,1))\n",
    "        tn = get_cell_from_confusion_matrix(reference_conf, (0,0))\n",
    "        fp = get_cell_from_confusion_matrix(reference_conf, (0,1))\n",
    "        fn = get_cell_from_confusion_matrix(reference_conf, (1,0))\n",
    "        reference_acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        reference_accuracies[reference_segment.key[0]] = reference_acc\n",
    "    \n",
    "\n",
    "    target_counts = get_segment_counts(target_results)\n",
    "    reference_counts = get_segment_counts(reference_results)\n",
    "\n",
    "    target_proportions = get_proportions(target_counts)\n",
    "    reference_proportions = get_proportions(reference_counts)\n",
    "\n",
    "    reference_accuracy  = estimate_accuracy_based_on_proportions(reference_accuracies, reference_proportions)\n",
    "    estimated_accuracy = estimate_accuracy_based_on_proportions(reference_accuracies, target_proportions)\n",
    "\n",
    "    return reference_accuracy, estimated_accuracy\n",
    "\n",
    "def get_segment_counts(results):\n",
    "    partition = results.partitions[0]\n",
    "    segments = results.segments_in_partition(partition)\n",
    "    segmented_column = partition.name\n",
    "\n",
    "    counts = {}\n",
    "    for segment in segments:\n",
    "        id = segment.parent_id\n",
    "        profile = results._segments[id][segment]\n",
    "        segment_count = profile._columns[segmented_column]._metrics['counts'].n.value\n",
    "        counts[segment.key[0]] = segment_count\n",
    "    return counts\n",
    "\n",
    "\n",
    "def estimate_accuracy_based_on_proportions(reference_accuracies, target_proportions):\n",
    "    estimated_accuracy = sum([reference_accuracies[k]*target_proportions[k] for k in reference_accuracies.keys()])\n",
    "    return estimated_accuracy\n",
    "\n",
    "def get_proportions(counts):\n",
    "    total = sum(counts.values())\n",
    "    proportions = {k: v/total for k, v in counts.items()}\n",
    "    return proportions\n",
    "\n",
    "def get_cell_from_confusion_matrix(confusion_matrix, key):\n",
    "    dist_cell = confusion_matrix.get(key,None)\n",
    "    return dist_cell.n if dist_cell is not None else 0\n",
    "\n",
    "df_perturbed = random_subsample_on_column(df, 'group', lower_pct=0.3, upper_pct=0.5,classes=['white','black', 'LGBTQ', 'muslim'])\n",
    "perturbed_results = log_with_metrics(df_perturbed)\n",
    "\n",
    "perturbed_acc = calculate_real_accuracy(df_perturbed)\n",
    "ref_acc = calculate_real_accuracy(df)\n",
    "reference_accuracy, estimated_accuracy = estimate_accuracy(results, perturbed_results)\n",
    "# print(f\"Reference accuracy: {reference_accuracy}\")\n",
    "# print(f\"Estimated accuracy: {estimated_accuracy}\")\n",
    "print(f\"Reference accuracy: {real_acc}\")\n",
    "print(f\"Perturbed accuracy: {perturbed_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideas\n",
    "\n",
    "from whylogs.experimental.performance_estimation import AccuracyEstimator\n",
    "\n",
    "estimator = AccuracyEstimator(reference_result_set = reference_results)\n",
    "\n",
    "perturbed_result_sets = [results_1, results_2, results_3]\n",
    "# or perturbed_dfs = [df_1, df_2, df_3]\n",
    "\n",
    "estimated_accuracies = [estimator.estimate_accuracy(results) for results in perturbed_result_set]\n",
    "# or estimated_accuracies = [estimator.estimate_accuracy(df) for df in perturbed_dfs]\n",
    "\n",
    "real_accuracies = [get_real_acc(df) for df in perturbed_dfs]\n",
    "\n",
    "plot_estimated_accuracies(estimated_accuracies, real_accuracies)\n",
    "\n",
    "# imagine beautiful plot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another idea\n",
    "\n",
    "# schema.estimator = estimator\n",
    "# results = why.log(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd5901cadfd4b29c2aaf95ecd29c0c3b10829ad94dcfe59437dbee391154aea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
